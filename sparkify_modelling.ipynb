{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to my EMR Notebook!\n"
     ]
    }
   ],
   "source": [
    "# this line is only used on AWS cluster\n",
    "print(\"Welcome to my EMR Notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%info` not found.\n"
     ]
    }
   ],
   "source": [
    "# this line is only used on AWS cluster\n",
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-26e9b9e3f83f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# this line is only used on AWS cluster\n",
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f7240b701244b4bf61fc50dfe9a7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==0.25.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/ab/ea76361f9d3e732e114adcd801d2820d5319c23d0ac5482fa3b412db217e/pandas-0.25.1-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==0.25.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==0.25.1)\n",
      "Collecting python-dateutil>=2.6.1 (from pandas==0.25.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==0.25.1)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-0.25.1 python-dateutil-2.8.1\n",
      "\n",
      "Collecting matplotlib\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/b9/712584f12f840968a14969a1fe298ffdeaa9c4b484b3bfd973c74c4a481d/matplotlib-3.3.1-cp37-cp37m-manylinux1_x86_64.whl (11.6MB)\n",
      "Collecting certifi>=2020.06.20 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib64/python3.7/site-packages (from matplotlib)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/f2/6722dd0c22e3a143ac792ccb2424924ac72af4adea756b1165b4cad50da7/Pillow-7.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/31/b9/6202dcae729998a0ade30e80ac00f616542ef445b088ec970d407dfd41c0/kiwisolver-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (88kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib)\n",
      "Installing collected packages: certifi, pyparsing, pillow, cycler, kiwisolver, matplotlib\n",
      "Successfully installed certifi-2020.6.20 cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.3.1 pillow-7.2.0 pyparsing-2.4.7\n",
      "\n",
      "Collecting seaborn==0.10.1\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/e6/54aaaafd0b87f51dfba92ba73da94151aa3bc179e5fe88fc5dfb3038e860/seaborn-0.10.1-py3-none-any.whl (215kB)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from seaborn==0.10.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from seaborn==0.10.1)\n",
      "Collecting scipy>=1.0.1 (from seaborn==0.10.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/f9/f7a7e5009711579c72da2725174825e5056741bf4001815d097eef1b2e17/scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9MB)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from seaborn==0.10.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn==0.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn==0.10.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn==0.10.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn==0.10.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn==0.10.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn==0.10.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn==0.10.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->seaborn==0.10.1)\n",
      "Installing collected packages: scipy, seaborn\n",
      "Successfully installed scipy-1.5.2 seaborn-0.10.1\n",
      "\n",
      "Collecting ipython==7.17.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/ba/0ea438e2acd68ce79fde9cf57b4b1f18386969d8a013cd549254b151dde1/ipython-7.17.0-py3-none-any.whl (786kB)\n",
      "Requirement already satisfied: setuptools>=18.5 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from ipython==7.17.0)\n",
      "Collecting decorator (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting traitlets>=4.2 (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/ab/872a23e29cec3cf2594af7e857f18b687ad21039c1f9b922fac5b9b142d5/traitlets-4.3.3-py2.py3-none-any.whl (75kB)\n",
      "Collecting pexpect; sys_platform != \"win32\" (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl (59kB)\n",
      "Collecting jedi>=0.10 (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/d4/36136b18daae06ad798966735f6c3fb96869c1be9f8245d2a8f556e40c36/jedi-0.17.2-py2.py3-none-any.whl (1.4MB)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/72/65/a3ef98b56d57a6d0a04cea5810ecbf3700a225d296ca298b3442dddebb42/prompt_toolkit-3.0.6-py3-none-any.whl (354kB)\n",
      "Collecting pickleshare (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/41/220f49aaea88bc6fa6cba8d05ecf24676326156c23b991e80b3f2fc24c77/pickleshare-0.7.5-py2.py3-none-any.whl\n",
      "Collecting backcall (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/1c/ff6546b6c12603d8dd1070aa3c3d273ad4c07f5771689a7b69a550e8c951/backcall-0.2.0-py2.py3-none-any.whl\n",
      "Collecting pygments (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/2d/68/106af3ae51daf807e9cdcba6a90e518954eb8b70341cee52995540a53ead/Pygments-2.6.1-py3-none-any.whl (914kB)\n",
      "Collecting ipython-genutils (from traitlets>=4.2->ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/bc/9bd3b5c2b4774d5f33b2d544f1460be9df7df2fe42f352135381c347c69a/ipython_genutils-0.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from traitlets>=4.2->ipython==7.17.0)\n",
      "Collecting ptyprocess>=0.5 (from pexpect; sys_platform != \"win32\"->ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whl\n",
      "Collecting parso<0.8.0,>=0.7.0 (from jedi>=0.10->ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/d1/e635bdde32890db5aeb2ffbde17e74f68986305a4466b0aa373b861e3f00/parso-0.7.1-py2.py3-none-any.whl (109kB)\n",
      "Collecting wcwidth (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/59/7c/e39aca596badaf1b78e8f547c807b04dae603a433d3e7a7e04d67f2ef3e5/wcwidth-0.2.5-py2.py3-none-any.whl\n",
      "Installing collected packages: decorator, ipython-genutils, traitlets, ptyprocess, pexpect, parso, jedi, wcwidth, prompt-toolkit, pickleshare, backcall, pygments, ipython\n",
      "Successfully installed backcall-0.2.0 decorator-4.4.2 ipython-7.17.0 ipython-genutils-0.2.0 jedi-0.17.2 parso-0.7.1 pexpect-4.8.0 pickleshare-0.7.5 prompt-toolkit-3.0.6 ptyprocess-0.6.0 pygments-2.6.1 traitlets-4.3.3 wcwidth-0.2.5"
     ]
    }
   ],
   "source": [
    "# those lines were only used on AWS cluster\n",
    "sc.install_pypi_package(\"pandas==0.25.1\") #Install pandas version 0.25.1 \n",
    "sc.install_pypi_package(\"matplotlib\") #Install matplotlib from given PyPI repository\n",
    "sc.install_pypi_package(\"seaborn==0.10.1\") #Install seaborn version 0.10.1\n",
    "sc.install_pypi_package(\"ipython==7.17.0\") #install ipython version 7.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'SparkContext' object has no attribute 'list_packages'\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'SparkContext' object has no attribute 'list_packages'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this line is only used on AWS cluster\n",
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession,Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression,GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1598254678553'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '42563'),\n",
       " ('spark.app.name', 'Sparkify'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.host', 'd55fb14ecc64'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session overview\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d55fb14ecc64:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sparkify</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0429052d68>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Dataset\n",
    "Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in full sparkify dataset\n",
    "#user_log = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\" #whole data on cloud\n",
    "#user_log = \"s3://aws-emr-resources-057584263306-eu-central-1/notebooks/e-1DYCDZU9IGL1MSTRN75J0HLSI/mini_sparkify_event_data.json\" #mini data on cloud\n",
    "user_log = \"mini_sparkify_event_data.json\" # mini data local\n",
    "df = spark.read.json(user_log)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|          artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|  Martha Tilston|Logged In|    Colin|     M|           50| Freeman|277.89016| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|           Rockpools|   200|1538352117000|Mozilla/5.0 (Wind...|    30|\n",
      "|Five Iron Frenzy|Logged In|    Micah|     M|           79|    Long|236.09424| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|              Canada|   200|1538352180000|\"Mozilla/5.0 (Win...|     9|\n",
      "|    Adam Lambert|Logged In|    Colin|     M|           51| Freeman| 282.8273| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|   Time For Miracles|   200|1538352394000|Mozilla/5.0 (Wind...|    30|\n",
      "|          Enigma|Logged In|    Micah|     M|           80|    Long|262.71302| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|Knocking On Forbi...|   200|1538352416000|\"Mozilla/5.0 (Win...|     9|\n",
      "|       Daft Punk|Logged In|    Colin|     M|           52| Freeman|223.60771| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|Harder Better Fas...|   200|1538352676000|Mozilla/5.0 (Wind...|    30|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show first 5 rows\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId| song|status| ts|userAgent|userId|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+\n",
      "| 58392|   0|     8346|  8346|            0|    8346| 58392|    0|    8346|     0|   0|        8346|        0|58392|     0|  0|     8346|     0|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Get count of both null and missing values in pyspark\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|      |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check \"userId\" with missing \"firstName\"\n",
    "df.select(['userId']).where(df.firstName.isNull()).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "|artist|      auth|firstName|gender|itemInSession|lastName|length|level|location|method| page|registration|sessionId|song|status|           ts|userAgent|userId|\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "|  null|Logged Out|     null|  null|          100|    null|  null| free|    null|   GET| Home|        null|        8|null|   200|1538355745000|     null|      |\n",
      "|  null|Logged Out|     null|  null|          101|    null|  null| free|    null|   GET| Help|        null|        8|null|   200|1538355807000|     null|      |\n",
      "|  null|Logged Out|     null|  null|          102|    null|  null| free|    null|   GET| Home|        null|        8|null|   200|1538355841000|     null|      |\n",
      "|  null|Logged Out|     null|  null|          103|    null|  null| free|    null|   PUT|Login|        null|        8|null|   307|1538355842000|     null|      |\n",
      "|  null|Logged Out|     null|  null|            2|    null|  null| free|    null|   GET| Home|        null|      240|null|   200|1538356678000|     null|      |\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show userId who's first name is missing\n",
    "#df.where(df.userId == \"1261737\").show() # on whole data\n",
    "df.where(df.userId == \"\").show(5) # on mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8346"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows with \"userId\" as 1261737\n",
    "#df.where(df.userId == \"1261737\").count() # on whole data\n",
    "df.where(df.userId == \"\").count() # on mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete rows with \"userId\" as 1261737\n",
    "#df_clean = df.filter(df.userId != \"1261737\") #on the whole data \n",
    "df_clean = df.filter(df.userId != \"\") #on the mini data \n",
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique userId \n",
    "df_clean.select('userId').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|    10|\n",
      "|   100|\n",
      "|100001|\n",
      "|100002|\n",
      "|100003|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show unique IDs\n",
    "df_clean.select(\"userId\").dropDuplicates().sort(\"userId\").show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+------+-------------+--------+------+-----+--------------------+------+---------------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "|artist|     auth|firstName|gender|itemInSession|lastName|length|level|            location|method|           page| registration|sessionId|song|status|           ts|           userAgent|userId|\n",
      "+------+---------+---------+------+-------------+--------+------+-----+--------------------+------+---------------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "|  null|Logged In|    Colin|     M|           54| Freeman|  null| paid|     Bakersfield, CA|   PUT|Add to Playlist|1538173362000|       29|null|   200|1538352905000|Mozilla/5.0 (Wind...|    30|\n",
      "|  null|Logged In|    Micah|     M|           84|    Long|  null| free|Boston-Cambridge-...|   GET|    Roll Advert|1538331630000|        8|null|   200|1538353150000|\"Mozilla/5.0 (Win...|     9|\n",
      "|  null|Logged In|    Micah|     M|           86|    Long|  null| free|Boston-Cambridge-...|   PUT|      Thumbs Up|1538331630000|        8|null|   307|1538353376000|\"Mozilla/5.0 (Win...|     9|\n",
      "|  null|Logged In|    Alexi|     F|            4|  Warren|  null| paid|Spokane-Spokane V...|   GET|      Downgrade|1532482662000|       53|null|   200|1538354749000|Mozilla/5.0 (Wind...|    54|\n",
      "|  null|Logged In|    Alexi|     F|            7|  Warren|  null| paid|Spokane-Spokane V...|   PUT|      Thumbs Up|1532482662000|       53|null|   307|1538355255000|Mozilla/5.0 (Wind...|    54|\n",
      "+------+---------+---------+------+-------------+--------+------+-----+--------------------+------+---------------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check rows with missing value in \"artist\" \n",
    "df_clean.where(df_clean.artist.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+---------------+-----+--------------------+\n",
      "|              artist|userId|firstname|           page|level|                song|\n",
      "+--------------------+------+---------+---------------+-----+--------------------+\n",
      "|      Martha Tilston|    30|    Colin|       NextSong| paid|           Rockpools|\n",
      "|        Adam Lambert|    30|    Colin|       NextSong| paid|   Time For Miracles|\n",
      "|           Daft Punk|    30|    Colin|       NextSong| paid|Harder Better Fas...|\n",
      "|        Starflyer 59|    30|    Colin|       NextSong| paid|Passengers (Old A...|\n",
      "|                null|    30|    Colin|Add to Playlist| paid|                null|\n",
      "|            Frumpies|    30|    Colin|       NextSong| paid|          Fuck Kitty|\n",
      "|Edward Sharpe & T...|    30|    Colin|       NextSong| paid|                Jade|\n",
      "|         Stan Mosley|    30|    Colin|       NextSong| paid|   So-Called Friends|\n",
      "|             Orishas|    30|    Colin|       NextSong| paid|           Represent|\n",
      "|            Downhere|    30|    Colin|       NextSong| paid|           Here I Am|\n",
      "|             Skillet|    30|    Colin|       NextSong| paid|Rebirthing (Album...|\n",
      "|Florence + The Ma...|    30|    Colin|       NextSong| paid|Dog Days Are Over...|\n",
      "|          Nick Drake|    30|    Colin|       NextSong| paid|Tomorrow Is A Lon...|\n",
      "|Lambert_ Hendrick...|    30|    Colin|       NextSong| paid|    Halloween Spooks|\n",
      "|          Kanye West|    30|    Colin|       NextSong| paid|            Stronger|\n",
      "|  Redman / Ready Roc|    30|    Colin|       NextSong| paid|   Dis Iz Brick City|\n",
      "|The All-American ...|    30|    Colin|       NextSong| paid|          Move Along|\n",
      "|       Kings Of Leon|    30|    Colin|       NextSong| paid|           Manhattan|\n",
      "|            BjÃÂ¶rk|    30|    Colin|       NextSong| paid|                Undo|\n",
      "|         The Prodigy|    30|    Colin|       NextSong| paid|     The Big Gundown|\n",
      "+--------------------+------+---------+---------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list activities of a user who has a missing value in the artist column 1009070\n",
    "# df_clean.select([\"artist\",\"userId\", \"firstname\", \"page\", \"level\", \"song\"]).where(df_clean.userId == \"1009070\").show() # on whole data\n",
    "df_clean.select([\"artist\",\"userId\", \"firstname\", \"page\", \"level\", \"song\"]).where(df_clean.userId == \"30\").show() # on mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                page|count|\n",
      "+--------------------+-----+\n",
      "|              Cancel|   52|\n",
      "|    Submit Downgrade|   63|\n",
      "|         Thumbs Down| 2546|\n",
      "|                Home|10082|\n",
      "|           Downgrade| 2055|\n",
      "|         Roll Advert| 3933|\n",
      "|              Logout| 3226|\n",
      "|       Save Settings|  310|\n",
      "|Cancellation Conf...|   52|\n",
      "|               About|  495|\n",
      "|            Settings| 1514|\n",
      "|     Add to Playlist| 6526|\n",
      "|          Add Friend| 4277|\n",
      "|           Thumbs Up|12551|\n",
      "|                Help| 1454|\n",
      "|             Upgrade|  499|\n",
      "|               Error|  252|\n",
      "|      Submit Upgrade|  159|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check page related to \"null\" artist\n",
    "df_clean.select('page').where(df_clean.artist.isNull()).groupBy('page').count().show()\n",
    "# there is no \"next song\" page in the listed page categories, artist name is \"null\" for all the other pages, except \"next song\" page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              artist|\n",
      "+--------------------+\n",
      "|                null|\n",
      "|                 !!!|\n",
      "|        & And Oceans|\n",
      "|'N Sync/Phil Collins|\n",
      "|        'Til Tuesday|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check artist column\n",
    "df_clean.select(\"artist\").dropDuplicates().sort(\"artist\").show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digging in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status| ts|userAgent|userId|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "|     0|   0|        0|     0|            0|       0|     0|    0|       0|     0|   0|           0|        0|   0|     0|  0|        0|     0|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228108"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete missing values in \"artist\" column\n",
    "df_no_null = df_clean.filter(df_clean.artist != \"\")\n",
    "\n",
    "### Get count of both null and missing values\n",
    "df_no_null.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_no_null.columns]).show() \n",
    "# there is no missing value in any column after deleting missing \"artist\"\n",
    "df_no_null.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique userId \n",
    "df_no_null.select('userId').dropDuplicates().count() # unique userId number did not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     auth|\n",
      "+---------+\n",
      "|Cancelled|\n",
      "|Logged In|\n",
      "+---------+\n",
      "\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|     M|\n",
      "+------+\n",
      "\n",
      "+-----+\n",
      "|level|\n",
      "+-----+\n",
      "| free|\n",
      "| paid|\n",
      "+-----+\n",
      "\n",
      "+------+\n",
      "|method|\n",
      "+------+\n",
      "|   PUT|\n",
      "|   GET|\n",
      "+------+\n",
      "\n",
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "|            Settings|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n",
      "+---------+\n",
      "|sessionId|\n",
      "+---------+\n",
      "|       29|\n",
      "|       26|\n",
      "|      474|\n",
      "|      964|\n",
      "|     1697|\n",
      "|     1806|\n",
      "|     2040|\n",
      "|     1950|\n",
      "|     2214|\n",
      "|      418|\n",
      "|       65|\n",
      "|      541|\n",
      "|      558|\n",
      "|     1010|\n",
      "|     1224|\n",
      "|     1277|\n",
      "|     1258|\n",
      "|     1360|\n",
      "|     1840|\n",
      "|     2173|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|   307|\n",
      "|   404|\n",
      "|   200|\n",
      "+------+\n",
      "\n",
      "+--------------------+\n",
      "|           userAgent|\n",
      "+--------------------+\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Win...|\n",
      "|Mozilla/5.0 (X11;...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|Mozilla/5.0 (Maci...|\n",
      "|Mozilla/5.0 (Wind...|\n",
      "|Mozilla/5.0 (Wind...|\n",
      "|Mozilla/5.0 (comp...|\n",
      "|\"Mozilla/5.0 (Win...|\n",
      "|Mozilla/5.0 (Maci...|\n",
      "|\"Mozilla/5.0 (Win...|\n",
      "|\"Mozilla/5.0 (iPh...|\n",
      "|\"Mozilla/5.0 (Win...|\n",
      "|Mozilla/5.0 (Wind...|\n",
      "|Mozilla/5.0 (comp...|\n",
      "|Mozilla/5.0 (comp...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Win...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|            location|\n",
      "+--------------------+\n",
      "|     Gainesville, FL|\n",
      "|Atlantic City-Ham...|\n",
      "|Deltona-Daytona B...|\n",
      "|San Diego-Carlsba...|\n",
      "|Cleveland-Elyria, OH|\n",
      "|Kingsport-Bristol...|\n",
      "|New Haven-Milford...|\n",
      "|Birmingham-Hoover...|\n",
      "|  Corpus Christi, TX|\n",
      "|         Dubuque, IA|\n",
      "|Las Vegas-Henders...|\n",
      "|Indianapolis-Carm...|\n",
      "|Seattle-Tacoma-Be...|\n",
      "|          Albany, OR|\n",
      "|   Winston-Salem, NC|\n",
      "|     Bakersfield, CA|\n",
      "|Los Angeles-Long ...|\n",
      "|Minneapolis-St. P...|\n",
      "|San Francisco-Oak...|\n",
      "|Phoenix-Mesa-Scot...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check tags in categorical columns\n",
    "for item in ['auth','gender','level','method','page','sessionId','status','userAgent','location']:\n",
    "    df_clean.select(item).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            location|\n",
      "+--------------------+\n",
      "|     Gainesville, FL|\n",
      "|Atlantic City-Ham...|\n",
      "|Deltona-Daytona B...|\n",
      "|San Diego-Carlsba...|\n",
      "|Cleveland-Elyria, OH|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check location column\n",
    "df_clean.select(\"location\").dropDuplicates().show(5) \n",
    "df_clean.select(\"location\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify \"location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column \"state\" with Abbreviation of US states extracted from \"location\" column\n",
    "df_clean = df_clean.withColumn('state', trim(split(col('location'),',').getItem(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique \"state\" \n",
    "df_clean.select('state').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist',\n",
       " 'auth',\n",
       " 'firstName',\n",
       " 'gender',\n",
       " 'itemInSession',\n",
       " 'lastName',\n",
       " 'length',\n",
       " 'level',\n",
       " 'location',\n",
       " 'method',\n",
       " 'page',\n",
       " 'registration',\n",
       " 'sessionId',\n",
       " 'song',\n",
       " 'status',\n",
       " 'ts',\n",
       " 'userAgent',\n",
       " 'userId',\n",
       " 'state']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df_clean = df_clean.drop('location','auth','firstName','lastName','method','status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'epoch time' from timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ts to epoch time and create month, date, hour columns out of it\n",
    "df_clean = df_clean.withColumn('epoch_time', from_unixtime(col('ts').cast(LongType())/1000).cast(TimestampType()))\n",
    "df_clean = df_clean.withColumn('date', from_unixtime(col('ts')/1000).cast(DateType()))\n",
    "#df_clean = df_clean.withColumn('month', month(col('epoch_time')))\n",
    "#df_clean = df_clean.withColumn('hour', hour(col('epoch_time')))  \n",
    "#df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify \"userAgent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = udf(lambda x: str(re.findall(r'\\((.*?)\\)', x)[0].split(\";\")[0].split()[0]) if x is not None else None, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.withColumn(\"agent\", agent(df_clean.userAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop('userAgent')\n",
    "#df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Churn\n",
    "Create a column \"Churn\" to use as the label for model. Using the \"Cancellation Confirmation\" events to define churn, which happen for both paid and free users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+------+-----+-------------------------+-------------+---------+----+-------------+------+-----------+-------------------+----------+---------+\n",
      "|artist|gender|itemInSession|length|level|page                     |registration |sessionId|song|ts           |userId|state      |epoch_time         |date      |agent    |\n",
      "+------+------+-------------+------+-----+-------------------------+-------------+---------+----+-------------+------+-----------+-------------------+----------+---------+\n",
      "|null  |M     |104          |null  |paid |Cancellation Confirmation|1535623466000|514      |null|1538943990000|18    |MO-KS      |2018-10-07 20:26:30|2018-10-07|Macintosh|\n",
      "|null  |M     |56           |null  |paid |Cancellation Confirmation|1537167593000|540      |null|1539033046000|32    |AZ         |2018-10-08 21:10:46|2018-10-08|iPhone   |\n",
      "|null  |M     |10           |null  |free |Cancellation Confirmation|1533157139000|174      |null|1539318918000|125   |TX         |2018-10-12 04:35:18|2018-10-12|Macintosh|\n",
      "|null  |M     |332          |null  |paid |Cancellation Confirmation|1536817381000|508      |null|1539375441000|105   |IN         |2018-10-12 20:17:21|2018-10-12|Windows  |\n",
      "|null  |F     |273          |null  |paid |Cancellation Confirmation|1538333829000|797      |null|1539465584000|17    |PA-NJ-DE-MD|2018-10-13 21:19:44|2018-10-13|Windows  |\n",
      "+------+------+-------------+------+-----+-------------------------+-------------+---------+----+-------------+------+-----------+-------------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find out users who churned their service\n",
    "df_clean.filter(\"page = 'Cancellation Confirmation'\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------+-----+--------------------+\n",
      "|userId|           ts|    page|level|                song|\n",
      "+------+-------------+--------+-----+--------------------+\n",
      "|    18|1538499917000|    Home| paid|                null|\n",
      "|    18|1538499933000|NextSong| paid|A Beggar On A Bea...|\n",
      "|    18|1538500208000|NextSong| paid|...slowdance On T...|\n",
      "|    18|1538500476000|NextSong| paid|       St. Apollonia|\n",
      "|    18|1538500654000|NextSong| paid|      Drunk Stripper|\n",
      "|    18|1538500842000|NextSong| paid|In League With Satan|\n",
      "|    18|1538500856000|Settings| paid|                null|\n",
      "|    18|1538501009000|NextSong| paid|           The Quest|\n",
      "|    18|1538501340000|NextSong| paid|    Waking The Demon|\n",
      "|    18|1538501587000|NextSong| paid|Why Do You Let Me...|\n",
      "|    18|1538501740000|NextSong| paid|High (Album Version)|\n",
      "|    18|1538501984000|NextSong| paid|Remember Me_ I'm ...|\n",
      "|    18|1538502021000|Settings| paid|                null|\n",
      "|    18|1538502124000|NextSong| paid|          Continents|\n",
      "|    18|1538502356000|NextSong| paid|               Girls|\n",
      "|    18|1538502626000|NextSong| paid|       What If I Do?|\n",
      "|    18|1538502928000|NextSong| paid|       Soulful Dress|\n",
      "|    18|1538503097000|NextSong| paid|        Eenie Meenie|\n",
      "|    18|1538503298000|NextSong| paid|          Kabul Shit|\n",
      "|    18|1538503521000|NextSong| paid|Catch You Baby (S...|\n",
      "+------+-------------+--------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list activities of a user who churned the service\n",
    "df_clean.select([\"userId\",\"ts\",\"page\", \"level\", \"song\"]).where(df_clean.userId == \"18\").show() #on mini data\n",
    "#df_clean.select([\"userId\", \"ts\", \"auth\",\"page\", \"level\"]).where(df_clean.userId == \"1768454\").sort(\"ts\").show() #on whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lable churn event,1 for churn, o for not\n",
    "flag_churn_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "df_clean = df_clean.withColumn(\"Churn\", flag_churn_event(\"page\"))\n",
    "\n",
    "# add churn user flag\n",
    "df_clean = df_clean.withColumn(\"label\", max('Churn').over(Window.partitionBy('UserId')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------+-----+-----+-----+\n",
      "|userId|           ts|    page|level|Churn|label|\n",
      "+------+-------------+--------+-----+-----+-----+\n",
      "|    18|1538499917000|    Home| paid|    0|    1|\n",
      "|    18|1538499933000|NextSong| paid|    0|    1|\n",
      "|    18|1538500208000|NextSong| paid|    0|    1|\n",
      "|    18|1538500476000|NextSong| paid|    0|    1|\n",
      "|    18|1538500654000|NextSong| paid|    0|    1|\n",
      "+------+-------------+--------+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list activities of a user who churned the service\n",
    "#df_clean.select([\"userId\", \"ts\",\"page\",\"level\",\"Churn\",\"label\"]).where(df_clean.userId == \"1768454\").show(5) #on whole data\n",
    "df_clean.select([\"userId\", \"ts\",\"page\",\"level\",\"Churn\",\"label\"]).where(df_clean.userId == \"18\").show(5) # on mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------+-------------+---------+-----+------+---------+-------------+---------+--------------------+------+-------------+------+-----+-------------------+----------+-----+------+-----+-----+\n",
      "|              artist|     auth|gender|itemInSession|   length|level|method|     page| registration|sessionId|                song|status|           ts|userId|state|         epoch_time|      date|month| agent|Churn|label|\n",
      "+--------------------+---------+------+-------------+---------+-----+------+---------+-------------+---------+--------------------+------+-------------+------+-----+-------------------+----------+-----+------+-----+-----+\n",
      "|Sleeping With Sirens|Logged In|     F|            0|202.97098| free|   PUT| NextSong|1538016340000|       31|Captain Tyin Knot...|   200|1539003534000|100010|   CT|2018-10-08 12:58:54|2018-10-08|   10|iPhone|    0|    0|\n",
      "|Francesca Battist...|Logged In|     F|            1|196.54485| free|   PUT| NextSong|1538016340000|       31|Beautiful_ Beauti...|   200|1539003736000|100010|   CT|2018-10-08 13:02:16|2018-10-08|   10|iPhone|    0|    0|\n",
      "|              Brutha|Logged In|     F|            2|263.13098| free|   PUT| NextSong|1538016340000|       31|          She's Gone|   200|1539003932000|100010|   CT|2018-10-08 13:05:32|2018-10-08|   10|iPhone|    0|    0|\n",
      "|                null|Logged In|     F|            3|     null| free|   PUT|Thumbs Up|1538016340000|       31|                null|   307|1539003933000|100010|   CT|2018-10-08 13:05:33|2018-10-08|   10|iPhone|    0|    0|\n",
      "|         Josh Ritter|Logged In|     F|            4|316.23791| free|   PUT| NextSong|1538016340000|       31|      Folk Bloodbath|   200|1539004195000|100010|   CT|2018-10-08 13:09:55|2018-10-08|   10|iPhone|    0|    0|\n",
      "+--------------------+---------+------+-------------+---------+-----+------+---------+-------------+---------+--------------------+------+-------------+------+-----+-------------------+----------+-----+------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define downgrade \n",
    "\n",
    "Create a column \"downgraded\" using the \"Downgrade\" events. Flag users who downgraded at least once as 1 and users who never downgraded as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+-------------+------+-----+------+---------+-------------+---------+----+------+-------------+------+--------+-------------------+----------+-----+------+-----+-----+\n",
      "|artist|     auth|gender|itemInSession|length|level|method|     page| registration|sessionId|song|status|           ts|userId|   state|         epoch_time|      date|month| agent|Churn|label|\n",
      "+------+---------+------+-------------+------+-----+------+---------+-------------+---------+----+------+-------------+------+--------+-------------------+----------+-----+------+-----+-----+\n",
      "|  null|Logged In|     M|           55|  null| paid|   GET|Downgrade|1536269906000|      163|null|   200|1539857842000|200002|IL-IN-WI|2018-10-18 10:17:22|2018-10-18|   10|iPhone|    0|    0|\n",
      "+------+---------+------+-------------+------+-----+------+---------+-------------+---------+----+------+-------------+------+--------+-------------------+----------+-----+------+-----+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find out users who downgraded their service\n",
    "df_clean.filter(\"page = 'Downgrade'\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------+---------------+-----+\n",
      "|userId|           ts|     auth|           page|level|\n",
      "+------+-------------+---------+---------------+-----+\n",
      "|200002|1538393430000|Logged In|       NextSong| free|\n",
      "|200002|1538393476000|Logged In|    Roll Advert| free|\n",
      "|200002|1538393635000|Logged In|       NextSong| free|\n",
      "|200002|1538393806000|Logged In|       NextSong| free|\n",
      "|200002|1538393807000|Logged In|      Thumbs Up| free|\n",
      "|200002|1538393977000|Logged In|       NextSong| free|\n",
      "|200002|1538394156000|Logged In|Add to Playlist| free|\n",
      "|200002|1538501988000|Logged In|       NextSong| free|\n",
      "|200002|1538502256000|Logged In|       NextSong| free|\n",
      "|200002|1538502641000|Logged In|       NextSong| free|\n",
      "|200002|1538502773000|Logged In|       NextSong| free|\n",
      "|200002|1538502918000|Logged In|    Roll Advert| free|\n",
      "|200002|1538502945000|Logged In|       NextSong| free|\n",
      "|200002|1538503168000|Logged In|       NextSong| free|\n",
      "|200002|1538503241000|Logged In|          About| free|\n",
      "|200002|1538503242000|Logged In|     Add Friend| free|\n",
      "|200002|1538503248000|Logged In|           Home| free|\n",
      "|200002|1538503391000|Logged In|       Settings| free|\n",
      "|200002|1538503405000|Logged In|       NextSong| free|\n",
      "|200002|1538503456000|Logged In|           Home| free|\n",
      "+------+-------------+---------+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list activities of a user who downgraded the service\n",
    "#df_clean.select([\"userId\", \"ts\", \"auth\",\"page\", \"level\"]).where(df_clean.userId == \"1001393\").sort(\"ts\").show() # on whole data\n",
    "df_clean.select([\"userId\", \"ts\", \"auth\",\"page\", \"level\"]).where(df_clean.userId == \"200002\").sort(\"ts\").show() # on mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lable downgrade event,1 for downgraded, 0 for not\n",
    "flag_downgrade_event = udf(lambda x: 1 if x == \"Downgrade\" else 0, IntegerType())\n",
    "df_clean = df_clean.withColumn(\"Downgrade\", flag_downgrade_event(\"page\"))\n",
    "\n",
    "# add downgraded user flag\n",
    "df_clean = df_clean.withColumn(\"downgraded\", max('Downgrade').over(Window.partitionBy('UserId')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+-----+----------+-----+\n",
      "|userId|     auth|           page|level|downgraded|label|\n",
      "+------+---------+---------------+-----+----------+-----+\n",
      "|200002|Logged In|       NextSong| free|         1|    0|\n",
      "|200002|Logged In|    Roll Advert| free|         1|    0|\n",
      "|200002|Logged In|       NextSong| free|         1|    0|\n",
      "|200002|Logged In|       NextSong| free|         1|    0|\n",
      "|200002|Logged In|      Thumbs Up| free|         1|    0|\n",
      "|200002|Logged In|       NextSong| free|         1|    0|\n",
      "|200002|Logged In|Add to Playlist| free|         1|    0|\n",
      "|200002|Logged In|       NextSong| free|         1|    0|\n",
      "|200002|Logged In|       NextSong| free|         1|    0|\n",
      "|200002|Logged In|       NextSong| free|         1|    0|\n",
      "|200002|Logged In|       NextSong| free|         1|    0|\n",
      "|200002|Logged In|    Roll Advert| free|         1|    0|\n",
      "|200002|Logged In|       NextSong| free|         1|    0|\n",
      "|200002|Logged In|       NextSong| free|         1|    0|\n",
      "|200002|Logged In|          About| free|         1|    0|\n",
      "|200002|Logged In|     Add Friend| free|         1|    0|\n",
      "|200002|Logged In|           Home| free|         1|    0|\n",
      "|200002|Logged In|       Settings| free|         1|    0|\n",
      "|200002|Logged In|       NextSong| free|         1|    0|\n",
      "|200002|Logged In|           Home| free|         1|    0|\n",
      "+------+---------+---------------+-----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list activities of a user who downgraded the service\n",
    "#df_clean.select([\"userId\", \"auth\",\"page\", \"level\", \"downgraded\",\"label\"]).where(df_clean.userId == \"1001393\").sort(\"ts\").show() # on whole data\n",
    "df_clean.select([\"userId\", \"auth\",\"page\", \"level\", \"downgraded\",\"label\"]).where(df_clean.userId == \"200002\").sort(\"ts\").show() # on whole data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 'Downgrade','Churn' columns\n",
    "df_clean = df_clean.drop('Downgrade','Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist',\n",
       " 'gender',\n",
       " 'itemInSession',\n",
       " 'length',\n",
       " 'level',\n",
       " 'page',\n",
       " 'registration',\n",
       " 'sessionId',\n",
       " 'song',\n",
       " 'ts',\n",
       " 'userId',\n",
       " 'state',\n",
       " 'epoch_time',\n",
       " 'date',\n",
       " 'agent',\n",
       " 'label',\n",
       " 'downgraded']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data\n",
    "df_page = df_clean.select('label','userId','page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+------+-----+----+------------+---------+----+---+------+-----+----------+----+-----+-----+----------+\n",
      "|artist|gender|itemInSession|length|level|page|registration|sessionId|song| ts|userId|state|epoch_time|date|agent|label|downgraded|\n",
      "+------+------+-------------+------+-----+----+------------+---------+----+---+------+-----+----------+----+-----+-----+----------+\n",
      "|     0|     0|            0|     0|    0|   0|           0|        0|   0|  0|     0|    0|         0|   0|    0|    0|         0|\n",
      "+------+------+-------------+------+-----+----+------------+---------+----+---+------+-----+----------+----+-----+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228108"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create smaller data\n",
    "# delete missing values in \"artist\" column\n",
    "df_clean = df_clean.filter(df_clean.artist != \"\")\n",
    "\n",
    "### Get count of missing values\n",
    "df_clean.select([count(when(col(c).isNull(), c)).alias(c) for c in df_clean.columns]).show() \n",
    "\n",
    "# there is no missing value in any column after deleting missing \"artist\"\n",
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique userId \n",
    "df_clean.select('userId').dropDuplicates().count() # unique userId number did not change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data (plotting codes are separated to another file because of memory overuse problem)\n",
    "Perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. Exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registered days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancellation_df = df_clean.select('userId','ts').groupBy('userId').agg(max('ts').alias('lastinteraction'))\n",
    "df_clean = cancellation_df.join(df_clean, on='userId').withColumn('registered_days', ((col('lastinteraction')-col('registration'))/86400000).cast(IntegerType()))\n",
    "df_clean = df_clean.drop('lastinteraction','registration')\n",
    "#df_clean.show(2)\n",
    "del cancellation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latest \"level\" of users before they churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_df = df_clean.select('ts','userId','level').orderBy('ts', ascending=False).groupBy('userId').agg(first('level').alias('valid_level'))    \n",
    "df_clean = df_clean.drop('level')\n",
    "df_clean = df_clean.join(level_df, on='userId')\n",
    "#df_clean.show(2)\n",
    "del level_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of songs per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+-------------+---------+--------+---------+--------------------+-------------+-----+-------------------+----------+------+-----+----------+---------------+-----------+------------------+\n",
      "|userId|              artist|gender|itemInSession|   length|    page|sessionId|                song|           ts|state|         epoch_time|      date| agent|label|downgraded|registered_days|valid_level|    avg_daily_song|\n",
      "+------+--------------------+------+-------------+---------+--------+---------+--------------------+-------------+-----+-------------------+----------+------+-----+----------+---------------+-----------+------------------+\n",
      "|100010|Sleeping With Sirens|     F|            0|202.97098|NextSong|       31|Captain Tyin Knot...|1539003534000|   CT|2018-10-08 12:58:54|2018-10-08|iPhone|    0|         0|             55|       free|39.142857142857146|\n",
      "|100010|Francesca Battist...|     F|            1|196.54485|NextSong|       31|Beautiful_ Beauti...|1539003736000|   CT|2018-10-08 13:02:16|2018-10-08|iPhone|    0|         0|             55|       free|39.142857142857146|\n",
      "+------+--------------------+------+-------------+---------+--------+---------+--------------------+-------------+-----+-------------------+----------+------+-----+----------+---------------+-----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_daily_song = df_clean.select('userId','date','song').groupBy('userId','date').agg(countDistinct('song').alias('songs')).sort('userId')\n",
    "daily_song = temp_daily_song.groupBy('userId').avg('songs').withColumnRenamed('avg(songs)', 'avg_daily_song')\n",
    "df_clean = df_clean.join(daily_song, on='userId')\n",
    "del temp_daily_song,daily_song\n",
    "df_clean.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of songs per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature\n",
    "song_per_session_df = df_clean.select('page','label', 'userId', 'sessionId').where('page == \"NextSong\"').groupby(['label', 'userId', 'sessionId']).count()\\\n",
    "    .groupby(['label', 'userId']).agg({'count': 'avg'})\\\n",
    "    .withColumnRenamed('avg(count)', 'songs_per_session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join new feature to the dataframe\n",
    "df_clean = df_clean.join(song_per_session_df.drop('label'), on='userId')\n",
    "#df_clean.show(2)\n",
    "del song_per_session_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average session duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session duration for each user\n",
    "session_duration = df_clean.select('userId','sessionId','ts').groupBy('userId','sessionId').agg(((max('ts')-min('ts'))/1000/3600).alias('activesession'))\n",
    "# average session duration for each user\n",
    "session_duration_df = session_duration.groupBy('userId').avg('activesession').withColumnRenamed('avg(activesession)', 'avg_session')\n",
    "\n",
    "# join dataframe to create new column\n",
    "df_clean = df_clean.join(session_duration_df, on='userId')\n",
    "#df_clean.show(2)\n",
    "del session_duration,session_duration_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friends added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total number of \"add friend\"\n",
    "friends_df = df_page.where('page == \"Add Friend\"').groupby(['label', 'userId']).count()\\\n",
    "    .groupby(['label', 'userId']).agg({'count': 'avg'})\\\n",
    "    .withColumnRenamed('avg(count)', 'friends')\n",
    "#friends_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframe to create new column \"friends\"\n",
    "df_clean = df_clean.join(friends_df.drop('label'), on='userId',how='left')\n",
    "#df_clean.show(2)\n",
    "del friends_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of \"thumbs up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total number of \"thumbs up\"\n",
    "thumbs_up_df = df_page.where('page == \"Thumbs Up\"').groupby(['label','userId']).count()\\\n",
    "    .groupby(['label','userId']).agg({'count': 'avg'})\\\n",
    "    .withColumnRenamed('avg(count)','thumbs_ups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframe to create new column \"thumbs_ups\"\n",
    "df_clean = df_clean.join(thumbs_up_df.drop('label'), on='userId',how='left')\n",
    "#df_clean.show(2)\n",
    "del thumbs_up_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of \"thumbs down\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total number of \"thumbs down\"\n",
    "thumbs_down_df = df_page.where('page == \"Thumbs Down\"').groupby(['label','userId']).count()\\\n",
    "    .groupby(['label','userId']).agg({'count': 'avg'})\\\n",
    "    .withColumnRenamed('avg(count)','thumbs_downs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframe to create new column \"thumbs_down\"\n",
    "df_clean = df_clean.join(thumbs_down_df.drop('label'), on='userId',how='left')\n",
    "#df_clean.show(2)\n",
    "del thumbs_down_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Times \"add to playlist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of \"add to playlist\"\n",
    "add_playlist_df = df_page.where('page == \"Add to Playlist\"').groupby(['label', 'userId']).count()\\\n",
    "    .groupby(['label', 'userId']).agg({'count': 'avg'})\\\n",
    "    .withColumnRenamed('avg(count)', 'add_playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframe to create new column \"add_playlist\"\n",
    "df_clean = df_clean.join(add_playlist_df.drop('label'), on='userId',how='left')\n",
    "#df_clean.show(2)\n",
    "del add_playlist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Times \"Roll Advert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of \"roll advert\"\n",
    "roll_advert_df = df_page.where('page == \"Add to Playlist\"').groupby(['label', 'userId']).count()\\\n",
    "    .groupby(['label', 'userId']).agg({'count': 'avg'})\\\n",
    "    .withColumnRenamed('avg(count)', 'roll_advert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframe to create new column \"roll_advert\"\n",
    "df_clean = df_clean.join(roll_advert_df.drop('label'), on='userId',how='left')\n",
    "#df_clean.show(2)\n",
    "del roll_advert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId',\n",
       " 'artist',\n",
       " 'gender',\n",
       " 'itemInSession',\n",
       " 'length',\n",
       " 'page',\n",
       " 'sessionId',\n",
       " 'song',\n",
       " 'ts',\n",
       " 'state',\n",
       " 'epoch_time',\n",
       " 'date',\n",
       " 'agent',\n",
       " 'label',\n",
       " 'downgraded',\n",
       " 'registered_days',\n",
       " 'valid_level',\n",
       " 'avg_daily_song',\n",
       " 'songs_per_session',\n",
       " 'avg_session',\n",
       " 'friends',\n",
       " 'thumbs_ups',\n",
       " 'thumbs_downs',\n",
       " 'add_playlist',\n",
       " 'roll_advert']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for visualization\n",
    "df_clean.select('userId','label','downgraded','gender','valid_level','agent','state',\\\n",
    "          'registered_days','avg_daily_song','songs_per_session','avg_session',\\\n",
    "          'friends','thumbs_ups','thumbs_downs','add_playlist','roll_advert').dropDuplicates(['userId']).write.save(\"df_visual\", format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble features for modeling\n",
    "df_model = df_clean.select(['userId','label','downgraded','gender','valid_level','agent',\\\n",
    "                            'registered_days','avg_daily_song','songs_per_session','avg_session',\\\n",
    "                            'friends','thumbs_ups','thumbs_downs','add_playlist','roll_advert']).dropDuplicates(['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+------+-----------+-----+---------------+--------------+-----------------+-----------+-------+----------+------------+------------+-----------+\n",
      "|userId|label|downgraded|gender|valid_level|agent|registered_days|avg_daily_song|songs_per_session|avg_session|friends|thumbs_ups|thumbs_downs|add_playlist|roll_advert|\n",
      "+------+-----+----------+------+-----------+-----+---------------+--------------+-----------------+-----------+-------+----------+------------+------------+-----------+\n",
      "|     0|    0|         0|     0|          0|    0|              0|             0|                0|          0|     19|         5|          22|          10|         10|\n",
      "+------+-----+----------+------+-----------+-----+---------------+--------------+-----------------+-----------+-------+----------+------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Get count of missing values\n",
    "df_model.select([count(when(col(c).isNull(), c)).alias(c) for c in df_model.columns]).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId',\n",
       " 'label',\n",
       " 'downgraded',\n",
       " 'gender',\n",
       " 'valid_level',\n",
       " 'agent',\n",
       " 'registered_days',\n",
       " 'avg_daily_song',\n",
       " 'songs_per_session',\n",
       " 'avg_session',\n",
       " 'friends',\n",
       " 'thumbs_ups',\n",
       " 'thumbs_downs',\n",
       " 'add_playlist',\n",
       " 'roll_advert',\n",
       " 'daily_song',\n",
       " 'session_song',\n",
       " 'session_duration']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format decimal\n",
    "df_model = df_model.withColumn(\"daily_song\", bround(df_model.avg_daily_song,1))\n",
    "df_model = df_model.withColumn(\"session_song\", bround(df_model.songs_per_session,1))\n",
    "df_model = df_model.withColumn(\"session_duration\", bround(df_model.avg_session,1))\n",
    "\n",
    "# replace missing value with 0\n",
    "df_model = df_model.na.fill(0)\n",
    "df_model.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop old columns\n",
    "df_model = df_model.drop('avg_daily_song','songs_per_session','avg_session','userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+-----------+---------+---------------+-------+----------+------------+------------+-----------+----------+------------+----------------+\n",
      "|label|downgraded|gender|valid_level|    agent|registered_days|friends|thumbs_ups|thumbs_downs|add_playlist|roll_advert|daily_song|session_song|session_duration|\n",
      "+-----+----------+------+-----------+---------+---------------+-------+----------+------------+------------+-----------+----------+------------+----------------+\n",
      "|    0|         0|     F|       free|   iPhone|             55|    4.0|      17.0|         5.0|         7.0|        7.0|      39.1|        39.3|             2.6|\n",
      "|    0|         1|     M|       paid|   iPhone|             70|    4.0|      21.0|         6.0|         8.0|        8.0|      55.0|        64.5|             4.4|\n",
      "|    1|         0|     M|       free|Macintosh|             71|    0.0|       0.0|         0.0|         0.0|        0.0|       8.0|         8.0|             0.5|\n",
      "|    0|         1|     F|       paid|Macintosh|            131|   74.0|     171.0|        41.0|       118.0|      118.0|     121.1|       145.7|             9.6|\n",
      "|    1|         1|     M|       paid|  Windows|             19|   28.0|     100.0|        21.0|        52.0|       52.0|     158.5|       211.1|            14.5|\n",
      "+-----+----------+------+-----------+---------+---------------+-------+----------+------------+------------+-----------+----------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- downgraded: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- valid_level: string (nullable = true)\n",
      " |-- agent: string (nullable = true)\n",
      " |-- registered_days: integer (nullable = true)\n",
      " |-- friends: double (nullable = false)\n",
      " |-- thumbs_ups: double (nullable = false)\n",
      " |-- thumbs_downs: double (nullable = false)\n",
      " |-- add_playlist: double (nullable = false)\n",
      " |-- roll_advert: double (nullable = false)\n",
      " |-- daily_song: double (nullable = false)\n",
      " |-- session_song: double (nullable = false)\n",
      " |-- session_duration: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check columns\n",
    "df_model.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+-----------+-----+---------------+-------+----------+------------+------------+-----------+----------+------------+----------------+\n",
      "|label|downgraded|gender|valid_level|agent|registered_days|friends|thumbs_ups|thumbs_downs|add_playlist|roll_advert|daily_song|session_song|session_duration|\n",
      "+-----+----------+------+-----------+-----+---------------+-------+----------+------------+------------+-----------+----------+------------+----------------+\n",
      "|    0|         0|     0|          0|    0|              0|      0|         0|           0|           0|          0|         0|           0|               0|\n",
      "+-----+----------+------+-----------+-----+---------------+-------+----------+------------+------------+-----------+----------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Get count of missing values\n",
    "df_model.select([count(when(col(c).isNull(), c)).alias(c) for c in df_model.columns]).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for modeling\n",
    "df_model.write.save(\"df_model\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical columns with StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df_model) for column in ['gender','valid_level','agent'] ]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_r = pipeline.fit(df_model).transform(df_model)\n",
    "df_model = df_r.drop('gender','valid_level','agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------------+-------+----------+------------+------------+-----------+----------+------------+----------------+------------+-----------------+-----------+\n",
      "|label|downgraded|registered_days|friends|thumbs_ups|thumbs_downs|add_playlist|roll_advert|daily_song|session_song|session_duration|gender_index|valid_level_index|agent_index|\n",
      "+-----+----------+---------------+-------+----------+------------+------------+-----------+----------+------------+----------------+------------+-----------------+-----------+\n",
      "|    0|         0|             55|    4.0|      17.0|         5.0|         7.0|        7.0|      39.1|        39.3|             2.6|         1.0|              1.0|        2.0|\n",
      "|    0|         1|             70|    4.0|      21.0|         6.0|         8.0|        8.0|      55.0|        64.5|             4.4|         0.0|              0.0|        2.0|\n",
      "|    1|         0|             71|    0.0|       0.0|         0.0|         0.0|        0.0|       8.0|         8.0|             0.5|         0.0|              1.0|        1.0|\n",
      "|    0|         1|            130|   74.0|     171.0|        41.0|       118.0|      118.0|     124.9|       145.7|            10.0|         1.0|              0.0|        1.0|\n",
      "|    1|         1|             19|   28.0|     100.0|        21.0|        52.0|       52.0|     158.5|       211.1|            14.5|         0.0|              0.0|        0.0|\n",
      "+-----+----------+---------------+-------+----------+------------+------------+-----------+----------+------------+----------------+------------+-----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard sclarizer\n",
    "Use StandardScaler to scalerize the created “scaled_feature” column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'downgraded',\n",
       " 'registered_days',\n",
       " 'friends',\n",
       " 'thumbs_ups',\n",
       " 'thumbs_downs',\n",
       " 'add_playlist',\n",
       " 'roll_advert',\n",
       " 'daily_song',\n",
       " 'session_song',\n",
       " 'session_duration',\n",
       " 'gender_index',\n",
       " 'valid_level_index',\n",
       " 'agent_index']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|features                                           |\n",
      "+---------------------------------------------------+\n",
      "|[55.0,4.0,17.0,5.0,7.0,7.0,39.1,39.3,2.6]          |\n",
      "|[70.0,4.0,21.0,6.0,8.0,8.0,55.0,64.5,4.4]          |\n",
      "|(9,[0,6,7,8],[71.0,8.0,8.0,0.5])                   |\n",
      "|[131.0,74.0,171.0,41.0,118.0,118.0,121.1,145.7,9.6]|\n",
      "|[19.0,28.0,100.0,21.0,52.0,52.0,158.5,211.1,14.5]  |\n",
      "|[72.0,1.0,7.0,1.0,5.0,5.0,18.8,21.4,1.5]           |\n",
      "|[56.0,31.0,81.0,14.0,59.0,59.0,99.8,136.7,8.8]     |\n",
      "|[110.0,33.0,163.0,29.0,72.0,72.0,90.5,81.2,5.4]    |\n",
      "|[23.0,11.0,58.0,3.0,24.0,24.0,100.8,136.7,9.1]     |\n",
      "|[85.0,6.0,17.0,3.0,7.0,7.0,42.7,42.8,3.1]          |\n",
      "|[66.0,41.0,96.0,17.0,38.0,38.0,90.4,120.5,8.3]     |\n",
      "|[23.0,3.0,11.0,0.0,1.0,1.0,27.7,28.0,1.8]          |\n",
      "|[53.0,29.0,86.0,16.0,61.0,61.0,135.8,179.7,13.6]   |\n",
      "|[124.0,6.0,40.0,9.0,20.0,20.0,40.1,40.4,2.7]       |\n",
      "|[66.0,41.0,95.0,24.0,67.0,67.0,118.2,138.0,9.4]    |\n",
      "|[74.0,63.0,303.0,28.0,113.0,113.0,71.5,59.5,3.9]   |\n",
      "|[64.0,7.0,11.0,5.0,7.0,7.0,45.8,46.0,3.6]          |\n",
      "|[60.0,47.0,154.0,22.0,89.0,89.0,102.6,89.1,6.1]    |\n",
      "|[71.0,12.0,72.0,9.0,33.0,33.0,91.8,125.0,8.8]      |\n",
      "|[87.0,7.0,9.0,3.0,7.0,7.0,26.9,23.9,1.4]           |\n",
      "+---------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assembeling numeric features to create a vector\n",
    "cols=['registered_days','friends','thumbs_ups','thumbs_downs','add_playlist','roll_advert',\\\n",
    "      'daily_song','session_song','session_duration']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "\n",
    "# use the transform method to transform df\n",
    "df_model = assembler.transform(df_model)\n",
    "df_model.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Scaled_features                                                                                                                                                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[1.46095433566683,0.19434724774254747,0.2596294873653501,0.38233662155526527,0.21395831049077527,0.21395831049077527,1.2518757615061304,0.922113675073389,0.8826912851795525]  |\n",
      "|[1.859396427212329,0.19434724774254747,0.32071877851013836,0.4588039458663183,0.24452378341802888,0.24452378341802888,1.7609505596633548,1.5133926728303715,1.4937852518423198]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# standardize numeric feature vector\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "df_model = standardscaler.fit(df_model).transform(df_model)\n",
    "df_model.select(\"Scaled_features\").show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Scaled_features                                                                                                                                                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[1.461623603743267,0.19434724774254747,0.2596294873653501,0.38233662155526527,0.21395831049077527,0.21395831049077527,1.2465675015038198,0.922113675073389,0.8762953534378627] |\n",
      "|[1.8602482229459762,0.19434724774254747,0.32071877851013836,0.4588039458663183,0.24452378341802888,0.24452378341802888,1.7534836977675214,1.5133926728303715,1.482961367356383]|\n",
      "|(9,[0,6,7,8],[1.8868231975594902,0.2550521742207304,0.18770761833554994,0.16851833719958897])                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.select(\"Scaled_features\").show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all features in one single feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'downgraded',\n",
       " 'registered_days',\n",
       " 'friends',\n",
       " 'thumbs_ups',\n",
       " 'thumbs_downs',\n",
       " 'add_playlist',\n",
       " 'roll_advert',\n",
       " 'daily_song',\n",
       " 'session_song',\n",
       " 'session_duration',\n",
       " 'gender_index',\n",
       " 'valid_level_index',\n",
       " 'agent_index',\n",
       " 'features',\n",
       " 'Scaled_features']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['Scaled_features','downgraded','gender_index','valid_level_index','agent_index']\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol='exp_features')\n",
    "# use the transform method to transform df\n",
    "df_model = assembler.transform(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|exp_features                                                                                                                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[1.46095433566683,0.19434724774254747,0.2596294873653501,0.38233662155526527,0.21395831049077527,0.21395831049077527,1.2518757615061304,0.922113675073389,0.8826912851795525,0.0,1.0,1.0,2.0]    |\n",
      "|[1.859396427212329,0.19434724774254747,0.32071877851013836,0.4588039458663183,0.24452378341802888,0.24452378341802888,1.7609505596633548,1.5133926728303715,1.4937852518423198,1.0,0.0,0.0,2.0]  |\n",
      "|(13,[0,6,7,8,11,12],[1.8859592333153623,0.2561382632237607,0.18770761833554994,0.16974832407299087,1.0,1.0])                                                                                     |\n",
      "|[3.4797275994973584,3.5954240832371283,2.611567196439698,3.135160296753175,3.606725805415926,3.606725805415926,3.8772929595496777,3.418624998936203,3.2591678222014244,1.0,1.0,0.0,1.0]          |\n",
      "|[0.5046933159576321,1.3604307341978323,1.5272322786197063,1.605813810532114,1.5894045922171878,1.5894045922171878,5.074739340120759,4.953134778829324,4.922701398116735,1.0,0.0,0.0,0.0]         |\n",
      "|[1.9125220394183955,0.04858681193563687,0.10690625950337945,0.07646732431105305,0.15282736463626806,0.15282736463626806,0.6019249185758377,0.5021178790475961,0.5092449722189726,0.0,0.0,1.0,0.0]|\n",
      "|[1.4875171417698632,1.5061911700047428,1.2370581456819623,1.0705425403547426,1.803362902707963,1.803362902707963,3.195324833716415,3.2074539283087096,2.9875705036846396,1.0,0.0,0.0,0.0]        |\n",
      "|[2.92190867133366,1.6033647938760167,2.4893886141501214,2.2175524050205384,2.20071405076226,2.20071405076226,2.897564102718793,1.905232326105832,1.8332818999883016,1.0,1.0,0.0,0.0]             |\n",
      "|[0.6109445403697652,0.5344549312920055,0.8857947215994297,0.22940197293315914,0.7335713502540866,0.7335713502540866,3.227342116619385,3.2074539283087096,3.0894194981284335,1.0,1.0,0.0,0.0]     |\n",
      "|[2.257838518757828,0.29152087161382123,0.2596294873653501,0.22940197293315914,0.21395831049077527,0.21395831049077527,1.3671379799568228,1.0042357580951922,1.0524396092525434,1.0,0.0,0.0,0.0]  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.select('exp_features').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for modeling\n",
    "df_model.write.save(\"df_model_vector\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split\n",
    "As a first step break your data set into 80% of training data and set aside 20%. Set random seed to `42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest, validation = df_model.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"We have %d training examples and %d test examples.\" % (rest.count(), validation.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check imbalance in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of ones are 42\n",
      "Percentage of ones are 21.98952879581152\n"
     ]
    }
   ],
   "source": [
    "dataset_size = float(rest.select('label').count())\n",
    "numPositives = rest.select('label').where('label == 1').count()\n",
    "per_ones = (float(numPositives)/float(dataset_size))*100\n",
    "numNegatives = float(dataset_size-numPositives)\n",
    "print('The number of ones are {}'.format(numPositives))\n",
    "print('Percentage of ones are {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BalancingRatio = 0.7801047120418848\n"
     ]
    }
   ],
   "source": [
    "BalancingRatio= numNegatives/dataset_size\n",
    "print('BalancingRatio = {}'.format(BalancingRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      classWeights|\n",
      "+------------------+\n",
      "|0.2198952879581152|\n",
      "|0.7801047120418848|\n",
      "|0.2198952879581152|\n",
      "|0.7801047120418848|\n",
      "|0.7801047120418848|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a new column named “classWeights” in the “rest” dataset\n",
    "rest = rest.withColumn(\"classWeights\", when(rest.label == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "rest.select(\"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"exp_features\",weightCol=\"classWeights\",maxIter=10)\n",
    "model = lr.fit(rest)\n",
    "predict_rest = model.transform(rest)\n",
    "predict_val = model.transform(validation)\n",
    "predict_val.select(\"label\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+--------------------+\n",
      "|label|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|    0|[-0.6043482042729...|       1.0|[0.35334952437924...|\n",
      "|    0|[4.94462726323463...|       0.0|[0.99292878947127...|\n",
      "|    0|[1.20590043290124...|       0.0|[0.76957277421950...|\n",
      "|    1|[-0.3319441140787...|       1.0|[0.41776766558045...|\n",
      "|    0|[0.63986233410138...|       0.0|[0.65472234037888...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the LR model using BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "predict_val.select(\"label\",\"rawPrediction\",\"prediction\",\"probability\").show(5)\n",
    "#print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_rest)))\n",
    "#print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score on the train set is 78.51%\n",
      "The F1 score on the test set is 75.63%\n",
      "The areaUnderROC on the train set is 82.89%\n",
      "The areaUnderROC on the test set is 83.75%\n"
     ]
    }
   ],
   "source": [
    "#F1 score\n",
    "f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "f1_score_rest = f1_score_evaluator.evaluate(predict_rest.select(col('label'), col('prediction')))\n",
    "f1_score_val = f1_score_evaluator.evaluate(predict_val.select(col('label'), col('prediction')))\n",
    "print('The F1 score on the train set is {:.2%}'.format(f1_score_rest))\n",
    "print('The F1 score on the test set is {:.2%}'.format(f1_score_val)) \n",
    "\n",
    "#area under ROC \n",
    "auc_evaluator = BinaryClassificationEvaluator()\n",
    "roc_value_rest = auc_evaluator.evaluate(predict_rest, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "roc_value_val = auc_evaluator.evaluate(predict_val, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "print('The areaUnderROC on the train set is {:.2%}'.format(roc_value_rest))\n",
    "print('The areaUnderROC on the test set is {:.2%}'.format(roc_value_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"exp_features\")#,maxIter=10)#weightCol=\"classWeights\") \n",
    "model = rf.fit(rest)\n",
    "predict_rest = model.transform(rest)\n",
    "predict_val = model.transform(validation)\n",
    "predict_val.select(\"label\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+--------------------+\n",
      "|label|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|    0|[14.9518064118071...|       0.0|[0.74759032059035...|\n",
      "|    0|[18.6393729079922...|       0.0|[0.93196864539961...|\n",
      "|    0|[8.57377163122777...|       1.0|[0.42868858156138...|\n",
      "|    1|[18.3776037455221...|       0.0|[0.91888018727610...|\n",
      "|    0|[17.7137869358082...|       0.0|[0.88568934679041...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The F1 score on the train set is 90.85%\n",
      "The F1 score on the test set is 72.74%\n",
      "The areaUnderROC on the train set is 98.51%\n",
      "The areaUnderROC on the test set is 66.25%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the RF model using BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "predict_val.select(\"label\",\"rawPrediction\",\"prediction\",\"probability\").show(5)\n",
    "\n",
    "#F1 score\n",
    "f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "f1_score_rest = f1_score_evaluator.evaluate(predict_rest.select(col('label'), col('prediction')))\n",
    "f1_score_val = f1_score_evaluator.evaluate(predict_val.select(col('label'), col('prediction')))\n",
    "print('The F1 score on the train set is {:.2%}'.format(f1_score_rest))\n",
    "print('The F1 score on the test set is {:.2%}'.format(f1_score_val)) \n",
    "\n",
    "#area under ROC \n",
    "auc_evaluator = BinaryClassificationEvaluator()\n",
    "roc_value_rest = auc_evaluator.evaluate(predict_rest, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "roc_value_val = auc_evaluator.evaluate(predict_val, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "print('The areaUnderROC on the train set is {:.2%}'.format(roc_value_rest))\n",
    "print('The areaUnderROC on the test set is {:.2%}'.format(roc_value_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosted trees model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gradient-Boosted Tree object\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"exp_features\", maxIter=10)\n",
    "\n",
    "#Fit the model to the data\n",
    "model = gbt.fit(rest)\n",
    "\n",
    "# Score the training and testing dataset using fitted model for evaluation purposes\n",
    "predict_rest = model.transform(rest)\n",
    "predict_val = model.transform(validation)\n",
    "predict_val.select(\"label\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+--------------------+\n",
      "|label|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|    0|[1.24708207708245...|       0.0|[0.92373169149972...|\n",
      "|    0|[1.23999313964552...|       0.0|[0.92272681955453...|\n",
      "|    0|[-0.7454301479447...|       1.0|[0.18379263682647...|\n",
      "|    1|[-0.0790917318979...|       1.0|[0.46053638798985...|\n",
      "|    0|[0.87549905665144...|       0.0|[0.85207864900675...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The F1 score on the train set is 98.42%\n",
      "The F1 score on the test set is 81.73%\n",
      "The areaUnderROC on the train set is 99.92%\n",
      "The areaUnderROC on the test set is 77.08%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the gbt model using BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "predict_val.select(\"label\",\"rawPrediction\",\"prediction\",\"probability\").show(5)\n",
    "\n",
    "#F1 score\n",
    "f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "f1_score_rest = f1_score_evaluator.evaluate(predict_rest.select(col('label'), col('prediction')))\n",
    "f1_score_val = f1_score_evaluator.evaluate(predict_val.select(col('label'), col('prediction')))\n",
    "print('The F1 score on the train set is {:.2%}'.format(f1_score_rest))\n",
    "print('The F1 score on the test set is {:.2%}'.format(f1_score_val)) \n",
    "\n",
    "#area under ROC \n",
    "auc_evaluator = BinaryClassificationEvaluator()\n",
    "roc_value_rest = auc_evaluator.evaluate(predict_rest, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "roc_value_val = auc_evaluator.evaluate(predict_val, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "print('The areaUnderROC on the train set is {:.2%}'.format(roc_value_rest))\n",
    "print('The areaUnderROC on the test set is {:.2%}'.format(roc_value_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "Create a parameter grid for tuning the model and perform cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random forest model\n",
    "rfparamGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.maxDepth,[3,5,10])\\\n",
    "    .addGrid(rf.numTrees, [20,50,75])\\\n",
    "    .addGrid(rf.impurity,[\"entropy\", \"gini\"])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=rfparamGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(rest)\n",
    "\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "predict_rest = cvModel.transform(rest)\n",
    "predict_validation = cvModel.transform(validation)\n",
    "\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(predict_rest)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(predict_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Gradient-Boosted Trees model\n",
    "gbparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [2, 5, 10])\n",
    "             .addGrid(gbt.maxBins, [10, 20, 40])\n",
    "             .addGrid(gbt.maxIter, [5, 10, 20])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for train set after CV  is 0.8100031959092361\n",
      "The area under ROC for test set after CV  is 0.65625\n"
     ]
    }
   ],
   "source": [
    "# Create 3-fold CrossValidator\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=gbparamGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(rest)\n",
    "\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "predict_rest = cvModel.transform(rest)\n",
    "predict_validation = cvModel.transform(validation)\n",
    "\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(predict_rest)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(predict_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score on the train set is 84.22%\n",
      "The F1 score on the test set is 81.73%\n"
     ]
    }
   ],
   "source": [
    "#F1 score\n",
    "f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "f1_score_rest = f1_score_evaluator.evaluate(predict_rest.select(col('label'), col('prediction')))\n",
    "f1_score_val = f1_score_evaluator.evaluate(predict_val.select(col('label'), col('prediction')))\n",
    "print('The F1 score after CV on the train set is {:.2%}'.format(f1_score_rest))\n",
    "print('The F1 score after CV on the test set is {:.2%}'.format(f1_score_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work: Testing selective sampling method\n",
    "Because the training data set is imbalanced with more 0 labeled rows than 1, here I wanted to try if randomly selecting the same number of 0 rows as 1 rows might improve the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of stayed and churned users\n",
    "df_model.groupBy('label').agg(countDistinct('userId').alias('user_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random selection of same number of 0 labeled rows as the 1 labeled rows\n",
    "df_model.where(df_model.label == \"0\").orderBy(rand()).limit(57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
