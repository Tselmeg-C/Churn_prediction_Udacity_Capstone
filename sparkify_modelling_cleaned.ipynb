{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to my EMR Notebook!\n"
     ]
    }
   ],
   "source": [
    "# this line is only used on AWS cluster\n",
    "print(\"Welcome to my EMR Notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%info` not found.\n"
     ]
    }
   ],
   "source": [
    "# this line is only used on AWS cluster\n",
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line is only used on AWS cluster\n",
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f7240b701244b4bf61fc50dfe9a7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==0.25.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/ab/ea76361f9d3e732e114adcd801d2820d5319c23d0ac5482fa3b412db217e/pandas-0.25.1-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==0.25.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==0.25.1)\n",
      "Collecting python-dateutil>=2.6.1 (from pandas==0.25.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==0.25.1)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-0.25.1 python-dateutil-2.8.1\n",
      "\n",
      "Collecting matplotlib\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/b9/712584f12f840968a14969a1fe298ffdeaa9c4b484b3bfd973c74c4a481d/matplotlib-3.3.1-cp37-cp37m-manylinux1_x86_64.whl (11.6MB)\n",
      "Collecting certifi>=2020.06.20 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib64/python3.7/site-packages (from matplotlib)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/f2/6722dd0c22e3a143ac792ccb2424924ac72af4adea756b1165b4cad50da7/Pillow-7.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/31/b9/6202dcae729998a0ade30e80ac00f616542ef445b088ec970d407dfd41c0/kiwisolver-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (88kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib)\n",
      "Installing collected packages: certifi, pyparsing, pillow, cycler, kiwisolver, matplotlib\n",
      "Successfully installed certifi-2020.6.20 cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.3.1 pillow-7.2.0 pyparsing-2.4.7\n",
      "\n",
      "Collecting seaborn==0.10.1\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/e6/54aaaafd0b87f51dfba92ba73da94151aa3bc179e5fe88fc5dfb3038e860/seaborn-0.10.1-py3-none-any.whl (215kB)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from seaborn==0.10.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from seaborn==0.10.1)\n",
      "Collecting scipy>=1.0.1 (from seaborn==0.10.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/f9/f7a7e5009711579c72da2725174825e5056741bf4001815d097eef1b2e17/scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9MB)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from seaborn==0.10.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn==0.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn==0.10.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn==0.10.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn==0.10.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn==0.10.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn==0.10.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn==0.10.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->seaborn==0.10.1)\n",
      "Installing collected packages: scipy, seaborn\n",
      "Successfully installed scipy-1.5.2 seaborn-0.10.1\n",
      "\n",
      "Collecting ipython==7.17.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/ba/0ea438e2acd68ce79fde9cf57b4b1f18386969d8a013cd549254b151dde1/ipython-7.17.0-py3-none-any.whl (786kB)\n",
      "Requirement already satisfied: setuptools>=18.5 in /mnt/tmp/1597524246795-0/lib/python3.7/site-packages (from ipython==7.17.0)\n",
      "Collecting decorator (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting traitlets>=4.2 (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/ab/872a23e29cec3cf2594af7e857f18b687ad21039c1f9b922fac5b9b142d5/traitlets-4.3.3-py2.py3-none-any.whl (75kB)\n",
      "Collecting pexpect; sys_platform != \"win32\" (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl (59kB)\n",
      "Collecting jedi>=0.10 (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/d4/36136b18daae06ad798966735f6c3fb96869c1be9f8245d2a8f556e40c36/jedi-0.17.2-py2.py3-none-any.whl (1.4MB)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/72/65/a3ef98b56d57a6d0a04cea5810ecbf3700a225d296ca298b3442dddebb42/prompt_toolkit-3.0.6-py3-none-any.whl (354kB)\n",
      "Collecting pickleshare (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/41/220f49aaea88bc6fa6cba8d05ecf24676326156c23b991e80b3f2fc24c77/pickleshare-0.7.5-py2.py3-none-any.whl\n",
      "Collecting backcall (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/1c/ff6546b6c12603d8dd1070aa3c3d273ad4c07f5771689a7b69a550e8c951/backcall-0.2.0-py2.py3-none-any.whl\n",
      "Collecting pygments (from ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/2d/68/106af3ae51daf807e9cdcba6a90e518954eb8b70341cee52995540a53ead/Pygments-2.6.1-py3-none-any.whl (914kB)\n",
      "Collecting ipython-genutils (from traitlets>=4.2->ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/bc/9bd3b5c2b4774d5f33b2d544f1460be9df7df2fe42f352135381c347c69a/ipython_genutils-0.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from traitlets>=4.2->ipython==7.17.0)\n",
      "Collecting ptyprocess>=0.5 (from pexpect; sys_platform != \"win32\"->ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whl\n",
      "Collecting parso<0.8.0,>=0.7.0 (from jedi>=0.10->ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/d1/e635bdde32890db5aeb2ffbde17e74f68986305a4466b0aa373b861e3f00/parso-0.7.1-py2.py3-none-any.whl (109kB)\n",
      "Collecting wcwidth (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.17.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/59/7c/e39aca596badaf1b78e8f547c807b04dae603a433d3e7a7e04d67f2ef3e5/wcwidth-0.2.5-py2.py3-none-any.whl\n",
      "Installing collected packages: decorator, ipython-genutils, traitlets, ptyprocess, pexpect, parso, jedi, wcwidth, prompt-toolkit, pickleshare, backcall, pygments, ipython\n",
      "Successfully installed backcall-0.2.0 decorator-4.4.2 ipython-7.17.0 ipython-genutils-0.2.0 jedi-0.17.2 parso-0.7.1 pexpect-4.8.0 pickleshare-0.7.5 prompt-toolkit-3.0.6 ptyprocess-0.6.0 pygments-2.6.1 traitlets-4.3.3 wcwidth-0.2.5"
     ]
    }
   ],
   "source": [
    "# those lines were only used on AWS cluster\n",
    "sc.install_pypi_package(\"pandas==0.25.1\") #Install pandas version 0.25.1 \n",
    "sc.install_pypi_package(\"matplotlib\") #Install matplotlib from given PyPI repository\n",
    "sc.install_pypi_package(\"seaborn==0.10.1\") #Install seaborn version 0.10.1\n",
    "sc.install_pypi_package(\"ipython==7.17.0\") #install ipython version 7.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'SparkContext' object has no attribute 'list_packages'\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'SparkContext' object has no attribute 'list_packages'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this line is only used on AWS cluster\n",
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession,Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression,GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'Sparkify'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.host', '192.168.178.26'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '39087'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'local-1598356278490')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session overview\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.178.26:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sparkify</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f78c9ae5908>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Dataset\n",
    "Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in full sparkify dataset\n",
    "#user_log = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\" #whole data on cloud\n",
    "#user_log = \"s3://aws-emr-resources-057584263306-eu-central-1/notebooks/e-1DYCDZU9IGL1MSTRN75J0HLSI/mini_sparkify_event_data.json\" #mini data on cloud\n",
    "user_log = \"mini_sparkify_event_data.json\" # mini data local\n",
    "df = spark.read.json(user_log)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|          artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|  Martha Tilston|Logged In|    Colin|     M|           50| Freeman|277.89016| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|           Rockpools|   200|1538352117000|Mozilla/5.0 (Wind...|    30|\n",
      "|Five Iron Frenzy|Logged In|    Micah|     M|           79|    Long|236.09424| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|              Canada|   200|1538352180000|\"Mozilla/5.0 (Win...|     9|\n",
      "|    Adam Lambert|Logged In|    Colin|     M|           51| Freeman| 282.8273| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|   Time For Miracles|   200|1538352394000|Mozilla/5.0 (Wind...|    30|\n",
      "|          Enigma|Logged In|    Micah|     M|           80|    Long|262.71302| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|Knocking On Forbi...|   200|1538352416000|\"Mozilla/5.0 (Win...|     9|\n",
      "|       Daft Punk|Logged In|    Colin|     M|           52| Freeman|223.60771| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|Harder Better Fas...|   200|1538352676000|Mozilla/5.0 (Wind...|    30|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show first 5 rows\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.limit(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286500"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId| song|status| ts|userAgent|userId|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+\n",
      "| 58392|   0|     8346|  8346|            0|    8346| 58392|    0|    8346|     0|   0|        8346|        0|58392|     0|  0|     8346|     0|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get count of both null and missing values in pyspark\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|      |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check \"userId\" with missing \"firstName\"\n",
    "df.select(['userId']).where(df.firstName.isNull()).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "|artist|      auth|firstName|gender|itemInSession|lastName|length|level|location|method| page|registration|sessionId|song|status|           ts|userAgent|userId|\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "|  null|Logged Out|     null|  null|          100|    null|  null| free|    null|   GET| Home|        null|        8|null|   200|1538355745000|     null|      |\n",
      "|  null|Logged Out|     null|  null|          101|    null|  null| free|    null|   GET| Help|        null|        8|null|   200|1538355807000|     null|      |\n",
      "|  null|Logged Out|     null|  null|          102|    null|  null| free|    null|   GET| Home|        null|        8|null|   200|1538355841000|     null|      |\n",
      "|  null|Logged Out|     null|  null|          103|    null|  null| free|    null|   PUT|Login|        null|        8|null|   307|1538355842000|     null|      |\n",
      "|  null|Logged Out|     null|  null|            2|    null|  null| free|    null|   GET| Home|        null|      240|null|   200|1538356678000|     null|      |\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show userId who's first name is missing\n",
    "df.where(df.userId == \"\").show(5) # on mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8346"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows with \"userId\" as 1261737\n",
    "df.where(df.userId == \"\").count() # on mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete rows with \"userId\" as 1261737\n",
    "#df_clean = df.filter(df.userId != \"1261737\") #on the whole data \n",
    "df_clean = df.filter(df.userId != \"\") #on the mini data \n",
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique userId \n",
    "df_clean.select('userId').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|    10|\n",
      "|   100|\n",
      "|100001|\n",
      "|100002|\n",
      "|100003|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show unique IDs\n",
    "df_clean.select(\"userId\").dropDuplicates().sort(\"userId\").show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+------+-------------+--------+------+-----+--------------------+------+---------------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "|artist|     auth|firstName|gender|itemInSession|lastName|length|level|            location|method|           page| registration|sessionId|song|status|           ts|           userAgent|userId|\n",
      "+------+---------+---------+------+-------------+--------+------+-----+--------------------+------+---------------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "|  null|Logged In|    Colin|     M|           54| Freeman|  null| paid|     Bakersfield, CA|   PUT|Add to Playlist|1538173362000|       29|null|   200|1538352905000|Mozilla/5.0 (Wind...|    30|\n",
      "|  null|Logged In|    Micah|     M|           84|    Long|  null| free|Boston-Cambridge-...|   GET|    Roll Advert|1538331630000|        8|null|   200|1538353150000|\"Mozilla/5.0 (Win...|     9|\n",
      "|  null|Logged In|    Micah|     M|           86|    Long|  null| free|Boston-Cambridge-...|   PUT|      Thumbs Up|1538331630000|        8|null|   307|1538353376000|\"Mozilla/5.0 (Win...|     9|\n",
      "|  null|Logged In|    Alexi|     F|            4|  Warren|  null| paid|Spokane-Spokane V...|   GET|      Downgrade|1532482662000|       53|null|   200|1538354749000|Mozilla/5.0 (Wind...|    54|\n",
      "|  null|Logged In|    Alexi|     F|            7|  Warren|  null| paid|Spokane-Spokane V...|   PUT|      Thumbs Up|1532482662000|       53|null|   307|1538355255000|Mozilla/5.0 (Wind...|    54|\n",
      "+------+---------+---------+------+-------------+--------+------+-----+--------------------+------+---------------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check rows with missing value in \"artist\" \n",
    "df_clean.where(df_clean.artist.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+---------------+-----+--------------------+\n",
      "|              artist|userId|firstname|           page|level|                song|\n",
      "+--------------------+------+---------+---------------+-----+--------------------+\n",
      "|      Martha Tilston|    30|    Colin|       NextSong| paid|           Rockpools|\n",
      "|        Adam Lambert|    30|    Colin|       NextSong| paid|   Time For Miracles|\n",
      "|           Daft Punk|    30|    Colin|       NextSong| paid|Harder Better Fas...|\n",
      "|        Starflyer 59|    30|    Colin|       NextSong| paid|Passengers (Old A...|\n",
      "|                null|    30|    Colin|Add to Playlist| paid|                null|\n",
      "|            Frumpies|    30|    Colin|       NextSong| paid|          Fuck Kitty|\n",
      "|Edward Sharpe & T...|    30|    Colin|       NextSong| paid|                Jade|\n",
      "|         Stan Mosley|    30|    Colin|       NextSong| paid|   So-Called Friends|\n",
      "|             Orishas|    30|    Colin|       NextSong| paid|           Represent|\n",
      "|            Downhere|    30|    Colin|       NextSong| paid|           Here I Am|\n",
      "|             Skillet|    30|    Colin|       NextSong| paid|Rebirthing (Album...|\n",
      "|Florence + The Ma...|    30|    Colin|       NextSong| paid|Dog Days Are Over...|\n",
      "|          Nick Drake|    30|    Colin|       NextSong| paid|Tomorrow Is A Lon...|\n",
      "|Lambert_ Hendrick...|    30|    Colin|       NextSong| paid|    Halloween Spooks|\n",
      "|          Kanye West|    30|    Colin|       NextSong| paid|            Stronger|\n",
      "|  Redman / Ready Roc|    30|    Colin|       NextSong| paid|   Dis Iz Brick City|\n",
      "|The All-American ...|    30|    Colin|       NextSong| paid|          Move Along|\n",
      "|       Kings Of Leon|    30|    Colin|       NextSong| paid|           Manhattan|\n",
      "|            BjÃÂ¶rk|    30|    Colin|       NextSong| paid|                Undo|\n",
      "|         The Prodigy|    30|    Colin|       NextSong| paid|     The Big Gundown|\n",
      "+--------------------+------+---------+---------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list activities of a user who has a missing value in the artist column 1009070\n",
    "# df_clean.select([\"artist\",\"userId\", \"firstname\", \"page\", \"level\", \"song\"]).where(df_clean.userId == \"1009070\").show() # on whole data\n",
    "df_clean.select([\"artist\",\"userId\", \"firstname\", \"page\", \"level\", \"song\"]).where(df_clean.userId == \"30\").show() # on mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                page|count|\n",
      "+--------------------+-----+\n",
      "|              Cancel|   52|\n",
      "|    Submit Downgrade|   63|\n",
      "|         Thumbs Down| 2546|\n",
      "|                Home|10082|\n",
      "|           Downgrade| 2055|\n",
      "|         Roll Advert| 3933|\n",
      "|              Logout| 3226|\n",
      "|       Save Settings|  310|\n",
      "|Cancellation Conf...|   52|\n",
      "|               About|  495|\n",
      "|            Settings| 1514|\n",
      "|     Add to Playlist| 6526|\n",
      "|          Add Friend| 4277|\n",
      "|           Thumbs Up|12551|\n",
      "|                Help| 1454|\n",
      "|             Upgrade|  499|\n",
      "|               Error|  252|\n",
      "|      Submit Upgrade|  159|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check page related to \"null\" artist\n",
    "df_clean.select('page').where(df_clean.artist.isNull()).groupBy('page').count().show()\n",
    "# there is no \"next song\" page in the listed page categories, artist name is \"null\" for all the other pages, except \"next song\" page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              artist|\n",
      "+--------------------+\n",
      "|                null|\n",
      "|                 !!!|\n",
      "|        & And Oceans|\n",
      "|'N Sync/Phil Collins|\n",
      "|        'Til Tuesday|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check artist column\n",
    "df_clean.select(\"artist\").dropDuplicates().sort(\"artist\").show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digging in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status| ts|userAgent|userId|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "|     0|   0|        0|     0|            0|       0|     0|    0|       0|     0|   0|           0|        0|   0|     0|  0|        0|     0|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228108"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete missing values in \"artist\" column\n",
    "df_no_null = df_clean.filter(df_clean.artist != \"\")\n",
    "\n",
    "### Get count of both null and missing values\n",
    "df_no_null.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_no_null.columns]).show() \n",
    "# there is no missing value in any column after deleting missing \"artist\"\n",
    "df_no_null.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique userId \n",
    "df_no_null.select('userId').dropDuplicates().count() # unique userId number did not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     auth|\n",
      "+---------+\n",
      "|Cancelled|\n",
      "|Logged In|\n",
      "+---------+\n",
      "\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|     M|\n",
      "+------+\n",
      "\n",
      "+-----+\n",
      "|level|\n",
      "+-----+\n",
      "| free|\n",
      "| paid|\n",
      "+-----+\n",
      "\n",
      "+------+\n",
      "|method|\n",
      "+------+\n",
      "|   PUT|\n",
      "|   GET|\n",
      "+------+\n",
      "\n",
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "|            Settings|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n",
      "+---------+\n",
      "|sessionId|\n",
      "+---------+\n",
      "|       29|\n",
      "|       26|\n",
      "|      474|\n",
      "|      964|\n",
      "|     1697|\n",
      "|     1806|\n",
      "|     2040|\n",
      "|     1950|\n",
      "|     2214|\n",
      "|      418|\n",
      "|       65|\n",
      "|      541|\n",
      "|      558|\n",
      "|     1010|\n",
      "|     1224|\n",
      "|     1277|\n",
      "|     1258|\n",
      "|     1360|\n",
      "|     1840|\n",
      "|     2173|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|   307|\n",
      "|   404|\n",
      "|   200|\n",
      "+------+\n",
      "\n",
      "+--------------------+\n",
      "|           userAgent|\n",
      "+--------------------+\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Win...|\n",
      "|Mozilla/5.0 (X11;...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|Mozilla/5.0 (Maci...|\n",
      "|Mozilla/5.0 (Wind...|\n",
      "|Mozilla/5.0 (Wind...|\n",
      "|Mozilla/5.0 (comp...|\n",
      "|\"Mozilla/5.0 (Win...|\n",
      "|Mozilla/5.0 (Maci...|\n",
      "|\"Mozilla/5.0 (Win...|\n",
      "|\"Mozilla/5.0 (iPh...|\n",
      "|\"Mozilla/5.0 (Win...|\n",
      "|Mozilla/5.0 (Wind...|\n",
      "|Mozilla/5.0 (comp...|\n",
      "|Mozilla/5.0 (comp...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Win...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|            location|\n",
      "+--------------------+\n",
      "|     Gainesville, FL|\n",
      "|Atlantic City-Ham...|\n",
      "|Deltona-Daytona B...|\n",
      "|San Diego-Carlsba...|\n",
      "|Cleveland-Elyria, OH|\n",
      "|Kingsport-Bristol...|\n",
      "|New Haven-Milford...|\n",
      "|Birmingham-Hoover...|\n",
      "|  Corpus Christi, TX|\n",
      "|         Dubuque, IA|\n",
      "|Las Vegas-Henders...|\n",
      "|Indianapolis-Carm...|\n",
      "|Seattle-Tacoma-Be...|\n",
      "|          Albany, OR|\n",
      "|   Winston-Salem, NC|\n",
      "|     Bakersfield, CA|\n",
      "|Los Angeles-Long ...|\n",
      "|Minneapolis-St. P...|\n",
      "|San Francisco-Oak...|\n",
      "|Phoenix-Mesa-Scot...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check tags in categorical columns\n",
    "for item in ['auth','gender','level','method','page','sessionId','status','userAgent','location']:\n",
    "    df_clean.select(item).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            location|\n",
      "+--------------------+\n",
      "|     Gainesville, FL|\n",
      "|Atlantic City-Ham...|\n",
      "|Deltona-Daytona B...|\n",
      "|San Diego-Carlsba...|\n",
      "|Cleveland-Elyria, OH|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check location column\n",
    "df_clean.select(\"location\").dropDuplicates().show(5) \n",
    "df_clean.select(\"location\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify \"location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column \"state\" with Abbreviation of US states extracted from \"location\" column\n",
    "df_clean = df_clean.withColumn('state', trim(split(col('location'),',').getItem(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique \"state\" \n",
    "df_clean.select('state').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist',\n",
       " 'auth',\n",
       " 'firstName',\n",
       " 'gender',\n",
       " 'itemInSession',\n",
       " 'lastName',\n",
       " 'length',\n",
       " 'level',\n",
       " 'location',\n",
       " 'method',\n",
       " 'page',\n",
       " 'registration',\n",
       " 'sessionId',\n",
       " 'song',\n",
       " 'status',\n",
       " 'ts',\n",
       " 'userAgent',\n",
       " 'userId',\n",
       " 'state']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df_clean = df_clean.drop('location','auth','firstName','lastName','method','status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'epoch time' from timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ts to epoch time and create month, date, hour columns out of it\n",
    "df_clean = df_clean.withColumn('epoch_time', from_unixtime(col('ts').cast(LongType())/1000).cast(TimestampType()))\n",
    "df_clean = df_clean.withColumn('date', from_unixtime(col('ts')/1000).cast(DateType()))\n",
    "#df_clean = df_clean.withColumn('month', month(col('epoch_time')))\n",
    "#df_clean = df_clean.withColumn('hour', hour(col('epoch_time')))  \n",
    "#df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify \"userAgent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = udf(lambda x: str(re.findall(r'\\((.*?)\\)', x)[0].split(\";\")[0].split()[0]) if x is not None else None, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.withColumn(\"agent\", agent(df_clean.userAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop('userAgent')\n",
    "#df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Churn\n",
    "Create a column \"Churn\" to use as the label for model. Using the \"Cancellation Confirmation\" events to define churn, which happen for both paid and free users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\nTraceback (most recent call last):\n  File \"/home/narisu/anaconda3/envs/spark/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 3.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-c7798beca264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# find out users who churned their service\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"page = 'Cancellation Confirmation'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/spark/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spark/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spark/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spark/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\nTraceback (most recent call last):\n  File \"/home/narisu/anaconda3/envs/spark/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 3.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n"
     ]
    }
   ],
   "source": [
    "# find out users who churned their service\n",
    "df_clean.filter(\"page = 'Cancellation Confirmation'\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list activities of a user who churned the service\n",
    "df_clean.select([\"userId\",\"ts\",\"page\", \"level\", \"song\"]).where(df_clean.userId == \"18\").show() #on mini data\n",
    "#df_clean.select([\"userId\", \"ts\", \"auth\",\"page\", \"level\"]).where(df_clean.userId == \"1768454\").sort(\"ts\").show() #on whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lable churn event,1 for churn, o for not\n",
    "flag_churn_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "df_clean = df_clean.withColumn(\"Churn\", flag_churn_event(\"page\"))\n",
    "\n",
    "# add churn user flag\n",
    "df_clean = df_clean.withColumn(\"label\", max('Churn').over(Window.partitionBy('UserId')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list activities of a user who churned the service\n",
    "#df_clean.select([\"userId\", \"ts\",\"page\",\"level\",\"Churn\",\"label\"]).where(df_clean.userId == \"1768454\").show(5) #on whole data\n",
    "df_clean.select([\"userId\", \"ts\",\"page\",\"level\",\"Churn\",\"label\"]).where(df_clean.userId == \"18\").show(5) # on mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define downgrade \n",
    "\n",
    "Create a column \"downgraded\" using the \"Downgrade\" events. Flag users who downgraded at least once as 1 and users who never downgraded as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out users who downgraded their service\n",
    "df_clean.filter(\"page = 'Downgrade'\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list activities of a user who downgraded the service\n",
    "#df_clean.select([\"userId\", \"ts\", \"auth\",\"page\", \"level\"]).where(df_clean.userId == \"1001393\").sort(\"ts\").show() # on whole data\n",
    "df_clean.select([\"userId\", \"ts\", \"auth\",\"page\", \"level\"]).where(df_clean.userId == \"200002\").sort(\"ts\").show() # on mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lable downgrade event,1 for downgraded, 0 for not\n",
    "flag_downgrade_event = udf(lambda x: 1 if x == \"Downgrade\" else 0, IntegerType())\n",
    "df_clean = df_clean.withColumn(\"Downgrade\", flag_downgrade_event(\"page\"))\n",
    "\n",
    "# add downgraded user flag\n",
    "df_clean = df_clean.withColumn(\"downgraded\", max('Downgrade').over(Window.partitionBy('UserId')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list activities of a user who downgraded the service\n",
    "#df_clean.select([\"userId\", \"auth\",\"page\", \"level\", \"downgraded\",\"label\"]).where(df_clean.userId == \"1001393\").sort(\"ts\").show() # on whole data\n",
    "df_clean.select([\"userId\", \"auth\",\"page\", \"level\", \"downgraded\",\"label\"]).where(df_clean.userId == \"200002\").sort(\"ts\").show() # on whole data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 'Downgrade','Churn' columns\n",
    "df_clean = df_clean.drop('Downgrade','Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data\n",
    "df_page = df_clean.select('label','userId','page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller data\n",
    "# delete missing values in \"artist\" column\n",
    "df_clean = df_clean.filter(df_clean.artist != \"\")\n",
    "\n",
    "### Get count of missing values\n",
    "df_clean.select([count(when(col(c).isNull(), c)).alias(c) for c in df_clean.columns]).show() \n",
    "\n",
    "# there is no missing value in any column after deleting missing \"artist\"\n",
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique userId \n",
    "df_clean.select('userId').dropDuplicates().count() # unique userId number did not change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data (plotting codes are separated to another file because of memory overuse problem)\n",
    "Perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. Exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df_clean):\n",
    "    \"\"\"generate features based on df_clean\n",
    "    \n",
    "    Param:\n",
    "        df_clean: cleaned data frame from which all features are generated\n",
    "    \n",
    "    Return:\n",
    "        df_model: data frame with all necessary features\n",
    "    \"\"\"\n",
    "    \n",
    "    # add feature \"registered days\"\n",
    "    cancellation_df = df_clean.select('userId','ts')\\\n",
    "                              .groupBy('userId')\\\n",
    "                              .agg(max('ts')\\\n",
    "                              .alias('lastinteraction'))\n",
    "    df_clean = cancellation_df.join(df_clean, on='userId')\\\n",
    "                              .withColumn('registered_days', \n",
    "                                          ((col('lastinteraction')-col('registration'))/86400000)\n",
    "                              .cast(IntegerType()))\n",
    "    df_clean = df_clean.drop('lastinteraction','registration')\n",
    "    del cancellation_df\n",
    "    \n",
    "    # add feature \"latest level of users before they churn\"\n",
    "    level_df = df_clean.select('ts','userId','level')\\\n",
    "                       .orderBy('ts', ascending=False)\\\n",
    "                       .groupBy('userId')\\\n",
    "                       .agg(first('level')\\\n",
    "                       .alias('valid_level'))    \n",
    "    df_clean = df_clean.drop('level')\n",
    "    df_clean = df_clean.join(level_df, on='userId')\n",
    "    del level_df\n",
    "    \n",
    "    # add feature \"Number of songs per day\"\n",
    "    temp_daily_song = df_clean.select('userId','date','song')\\\n",
    "                              .groupBy('userId','date')\\\n",
    "                              .agg(countDistinct('song')\\\n",
    "                              .alias('songs'))\\\n",
    "                              .sort('userId')\n",
    "    daily_song = temp_daily_song.groupBy('userId')\\\n",
    "                                .avg('songs')\\\n",
    "                                .withColumnRenamed('avg(songs)',\n",
    "                                                   'avg_daily_song')\n",
    "    df_clean = df_clean.join(daily_song, on='userId')\n",
    "    del temp_daily_song,daily_song\n",
    "   \n",
    "    # add feature \"Number of songs per session\"\n",
    "    # create new feature\n",
    "    song_per_session_df = df_clean.select('page','label', 'userId', 'sessionId')\\\n",
    "                                  .where('page == \"NextSong\"')\\\n",
    "                                  .groupby(['label', 'userId', 'sessionId'])\\\n",
    "                                  .count()\\\n",
    "                                  .groupby(['label', 'userId'])\\\n",
    "                                  .agg({'count': 'avg'})\\\n",
    "                                  .withColumnRenamed('avg(count)', \n",
    "                                                     'songs_per_session')\n",
    "    df_clean = df_clean.join(song_per_session_df.drop('label'), on='userId')\n",
    "    del song_per_session_df\n",
    "    \n",
    "    # add feature \"verage session duration\"\n",
    "    # session duration for each user\n",
    "    session_duration = df_clean.select('userId','sessionId','ts')\\\n",
    "                               .groupBy('userId','sessionId')\\\n",
    "                               .agg(((max('ts')-min('ts'))/1000/3600)\\\n",
    "                               .alias('activesession'))\n",
    "    # average session duration for each user\n",
    "    session_duration_df = session_duration.groupBy('userId')\\\n",
    "                                          .avg('activesession')\\\n",
    "                                          .withColumnRenamed('avg(activesession)',\n",
    "                                                             'avg_session')\n",
    "    # join dataframe to create new column\n",
    "    df_clean = df_clean.join(session_duration_df, on='userId')\n",
    "    del session_duration,session_duration_df\n",
    "    \n",
    "    # add feature \"Friends added\"\n",
    "    # count total number of \"add friend\"\n",
    "    friends_df = df_page.where('page == \"Add Friend\"')\\\n",
    "                        .groupby(['label', 'userId'])\\\n",
    "                        .count()\\\n",
    "                        .groupby(['label', 'userId'])\\\n",
    "                        .agg({'count': 'avg'})\\\n",
    "                        .withColumnRenamed('avg(count)', \n",
    "                                           'friends')\n",
    "    # join dataframe to create new column \"friends\"\n",
    "    df_clean = df_clean.join(friends_df.drop('label'), on='userId',how='left')\n",
    "    del friends_df\n",
    "    \n",
    "    # add feature \"number of thumb ups\"\n",
    "    thumbs_up_df = df_page.where('page == \"Thumbs Up\"')\\\n",
    "                          .groupby(['label','userId'])\\\n",
    "                          .count()\\\n",
    "                          .groupby(['label','userId'])\\\n",
    "                          .agg({'count': 'avg'})\\\n",
    "                          .withColumnRenamed('avg(count)',\n",
    "                                             'thumbs_ups')\n",
    "    # join dataframe to create new column \"thumbs_ups\"\n",
    "    df_clean = df_clean.join(thumbs_up_df.drop('label'), on='userId',how='left')\n",
    "    del thumbs_up_df\n",
    "    \n",
    "    # add feature \"number of \"thumbs down\"\n",
    "    thumbs_down_df = df_page.where('page == \"Thumbs Down\"')\\\n",
    "                            .groupby(['label','userId'])\\\n",
    "                            .count()\\\n",
    "                            .groupby(['label','userId'])\\\n",
    "                            .agg({'count': 'avg'})\\\n",
    "                            .withColumnRenamed('avg(count)',\n",
    "                                               'thumbs_downs')\n",
    "    # join dataframe to create new column \"thumbs_down\"\n",
    "    df_clean = df_clean.join(thumbs_down_df.drop('label'), on='userId',how='left')\n",
    "    del thumbs_down_df\n",
    "    \n",
    "    # add feature \"Times to add to playlist\"\n",
    "    # count number of \"add to playlist\"\n",
    "    add_playlist_df = df_page.where('page == \"Add to Playlist\"')\\\n",
    "                             .groupby(['label', 'userId'])\\\n",
    "                             .count()\\\n",
    "                             .groupby(['label', 'userId'])\\\n",
    "                             .agg({'count': 'avg'})\\\n",
    "                             .withColumnRenamed('avg(count)',\n",
    "                                                'add_playlist')\n",
    "    # join dataframe to create new column \"add_playlist\"\n",
    "    df_clean = df_clean.join(add_playlist_df.drop('label'), on='userId',how='left')\n",
    "    del add_playlist_df\n",
    "    \n",
    "    # add feature \"times to roll advert\"\n",
    "    # count number of \"roll advert\"\n",
    "    roll_advert_df = df_page.where('page == \"Add to Playlist\"')\\\n",
    "                            .groupby(['label', 'userId'])\\\n",
    "                            .count()\\\n",
    "                            .groupby(['label', 'userId'])\\\n",
    "                            .agg({'count': 'avg'})\\\n",
    "                            .withColumnRenamed('avg(count)',\n",
    "                                               'roll_advert')\n",
    "\n",
    "    # join dataframe to create new column \"roll_advert\"\n",
    "    df_clean = df_clean.join(roll_advert_df.drop('label'), on='userId',how='left')\n",
    "    del roll_advert_df\n",
    "    \n",
    "    # assemble features for modeling\n",
    "    df_model = df_clean.select(['userId',\n",
    "                                   'label',\n",
    "                                   'downgraded',\n",
    "                                   'gender',\n",
    "                                   'valid_level',\n",
    "                                   'agent',\n",
    "                                   'registered_days',\n",
    "                                   'avg_daily_song',\n",
    "                                   'songs_per_session',\n",
    "                                   'avg_session',\n",
    "                                   'friends',\n",
    "                                   'thumbs_ups',\n",
    "                                   'thumbs_downs',\n",
    "                                   'add_playlist',\n",
    "                                   'roll_advert']).dropDuplicates(['userId'])\n",
    "    \n",
    "    # format decimal\n",
    "    df_model = df_model.withColumn(\"daily_song\", bround(df_model.avg_daily_song,1))\n",
    "    df_model = df_model.withColumn(\"session_song\", bround(df_model.songs_per_session,1))\n",
    "    df_model = df_model.withColumn(\"session_duration\", bround(df_model.avg_session,1))\n",
    "\n",
    "    # replace missing value with 0\n",
    "    df_model = df_model.na.fill(0)\n",
    "    \n",
    "    # drop old columns\n",
    "    df_model = df_model.drop('avg_daily_song','songs_per_session','avg_session','userId')\n",
    "\n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = prepare_features(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for modeling\n",
    "df_model.write.save(\"df_model\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform features for model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features(df_model):\n",
    "    \"\"\"transfrom the features into the form the model can digest\n",
    "    \n",
    "    Param:\n",
    "        df_model: the data frame with all necessary features.\n",
    "        \n",
    "    Return:\n",
    "        df_model: data data frame ready to be used for modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform categorical columns with StringIndexer\n",
    "    indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df_model) \n",
    "                    for column in ['gender','valid_level','agent'] ]\n",
    "    pipeline = Pipeline(stages=indexers)\n",
    "    df_r = pipeline.fit(df_model)\\\n",
    "                   .transform(df_model)\n",
    "    df_model = df_r.drop('gender','valid_level','agent')\n",
    "    \n",
    "    # use StandardScaler to scalerize the created “scaled_feature” column\n",
    "    # assembeling numeric features to create a vector\n",
    "    assembler = VectorAssembler(inputCols=['registered_days',\n",
    "                                           'friends',\n",
    "                                           'thumbs_ups',\n",
    "                                           'thumbs_downs',\n",
    "                                           'add_playlist',\n",
    "                                           'roll_advert',\n",
    "                                           'daily_song',\n",
    "                                           'session_song',\n",
    "                                           'session_duration'], \n",
    "                                outputCol=\"features\")\n",
    "    \n",
    "    # use the transform method to transform df\n",
    "    df_model = assembler.transform(df_model)\n",
    "    \n",
    "    # standardize numeric feature vector\n",
    "    standardscaler=StandardScaler().setInputCol(\"features\")\\\n",
    "                                   .setOutputCol(\"Scaled_features\")\n",
    "    df_model = standardscaler.fit(df_model)\\\n",
    "                             .transform(df_model)\n",
    " \n",
    "    # Combine all features in one single feature vector\n",
    "    assembler = VectorAssembler(inputCols=['Scaled_features',\n",
    "                                           'downgraded',\n",
    "                                           'gender_index',\n",
    "                                           'valid_level_index',\n",
    "                                           'agent_index'],\n",
    "                                outputCol='exp_features')\n",
    "    \n",
    "    # use the transform method to transform df\n",
    "    df_model = assembler.transform(df_model)\n",
    "    \n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = transform_features(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for modeling\n",
    "df_model.write.save(\"df_model_vector\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split\n",
    "As a first step break your data set into 80% of training data and set aside 20%. Set random seed to `42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest, validation = df_model.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"We have %d training examples and %d test examples.\" % (rest.count(), validation.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check imbalance in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = float(rest.select('label').count())\n",
    "numPositives = rest.select('label').where('label == 1').count()\n",
    "per_ones = (float(numPositives)/float(dataset_size))*100\n",
    "numNegatives = float(dataset_size-numPositives)\n",
    "print('The number of ones are {}'.format(numPositives))\n",
    "print('Percentage of ones are {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BalancingRatio= numNegatives/dataset_size\n",
    "print('BalancingRatio = {}'.format(BalancingRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column named “classWeights” in the “rest” dataset\n",
    "rest = rest.withColumn(\"classWeights\", when(rest.label == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "rest.select(\"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"exp_features\",weightCol=\"classWeights\",maxIter=10)\n",
    "model = lr.fit(rest)\n",
    "predict_rest = model.transform(rest)\n",
    "predict_val = model.transform(validation)\n",
    "predict_val.select(\"label\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+--------------------+\n",
      "|label|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|    0|[-0.6043482042729...|       1.0|[0.35334952437924...|\n",
      "|    0|[4.94462726323463...|       0.0|[0.99292878947127...|\n",
      "|    0|[1.20590043290124...|       0.0|[0.76957277421950...|\n",
      "|    1|[-0.3319441140787...|       1.0|[0.41776766558045...|\n",
      "|    0|[0.63986233410138...|       0.0|[0.65472234037888...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the LR model using BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "predict_val.select(\"label\",\"rawPrediction\",\"prediction\",\"probability\").show(5)\n",
    "#print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_rest)))\n",
    "#print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score on the train set is 78.51%\n",
      "The F1 score on the test set is 75.63%\n",
      "The areaUnderROC on the train set is 82.89%\n",
      "The areaUnderROC on the test set is 83.75%\n"
     ]
    }
   ],
   "source": [
    "#F1 score\n",
    "f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "f1_score_rest = f1_score_evaluator.evaluate(predict_rest.select(col('label'), col('prediction')))\n",
    "f1_score_val = f1_score_evaluator.evaluate(predict_val.select(col('label'), col('prediction')))\n",
    "print('The F1 score on the train set is {:.2%}'.format(f1_score_rest))\n",
    "print('The F1 score on the test set is {:.2%}'.format(f1_score_val)) \n",
    "\n",
    "#area under ROC \n",
    "auc_evaluator = BinaryClassificationEvaluator()\n",
    "roc_value_rest = auc_evaluator.evaluate(predict_rest, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "roc_value_val = auc_evaluator.evaluate(predict_val, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "print('The areaUnderROC on the train set is {:.2%}'.format(roc_value_rest))\n",
    "print('The areaUnderROC on the test set is {:.2%}'.format(roc_value_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"exp_features\")#,maxIter=10)#weightCol=\"classWeights\") \n",
    "model = rf.fit(rest)\n",
    "predict_rest = model.transform(rest)\n",
    "predict_val = model.transform(validation)\n",
    "predict_val.select(\"label\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+--------------------+\n",
      "|label|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|    0|[14.9518064118071...|       0.0|[0.74759032059035...|\n",
      "|    0|[18.6393729079922...|       0.0|[0.93196864539961...|\n",
      "|    0|[8.57377163122777...|       1.0|[0.42868858156138...|\n",
      "|    1|[18.3776037455221...|       0.0|[0.91888018727610...|\n",
      "|    0|[17.7137869358082...|       0.0|[0.88568934679041...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The F1 score on the train set is 90.85%\n",
      "The F1 score on the test set is 72.74%\n",
      "The areaUnderROC on the train set is 98.51%\n",
      "The areaUnderROC on the test set is 66.25%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the RF model using BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "predict_val.select(\"label\",\"rawPrediction\",\"prediction\",\"probability\").show(5)\n",
    "\n",
    "#F1 score\n",
    "f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "f1_score_rest = f1_score_evaluator.evaluate(predict_rest.select(col('label'), col('prediction')))\n",
    "f1_score_val = f1_score_evaluator.evaluate(predict_val.select(col('label'), col('prediction')))\n",
    "print('The F1 score on the train set is {:.2%}'.format(f1_score_rest))\n",
    "print('The F1 score on the test set is {:.2%}'.format(f1_score_val)) \n",
    "\n",
    "#area under ROC \n",
    "auc_evaluator = BinaryClassificationEvaluator()\n",
    "roc_value_rest = auc_evaluator.evaluate(predict_rest, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "roc_value_val = auc_evaluator.evaluate(predict_val, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "print('The areaUnderROC on the train set is {:.2%}'.format(roc_value_rest))\n",
    "print('The areaUnderROC on the test set is {:.2%}'.format(roc_value_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosted trees model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gradient-Boosted Tree object\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"exp_features\", maxIter=10)\n",
    "\n",
    "#Fit the model to the data\n",
    "model = gbt.fit(rest)\n",
    "\n",
    "# Score the training and testing dataset using fitted model for evaluation purposes\n",
    "predict_rest = model.transform(rest)\n",
    "predict_val = model.transform(validation)\n",
    "predict_val.select(\"label\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+--------------------+\n",
      "|label|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|    0|[1.24708207708245...|       0.0|[0.92373169149972...|\n",
      "|    0|[1.23999313964552...|       0.0|[0.92272681955453...|\n",
      "|    0|[-0.7454301479447...|       1.0|[0.18379263682647...|\n",
      "|    1|[-0.0790917318979...|       1.0|[0.46053638798985...|\n",
      "|    0|[0.87549905665144...|       0.0|[0.85207864900675...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The F1 score on the train set is 98.42%\n",
      "The F1 score on the test set is 81.73%\n",
      "The areaUnderROC on the train set is 99.92%\n",
      "The areaUnderROC on the test set is 77.08%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the gbt model using BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "predict_val.select(\"label\",\"rawPrediction\",\"prediction\",\"probability\").show(5)\n",
    "\n",
    "#F1 score\n",
    "f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "f1_score_rest = f1_score_evaluator.evaluate(predict_rest.select(col('label'), col('prediction')))\n",
    "f1_score_val = f1_score_evaluator.evaluate(predict_val.select(col('label'), col('prediction')))\n",
    "print('The F1 score on the train set is {:.2%}'.format(f1_score_rest))\n",
    "print('The F1 score on the test set is {:.2%}'.format(f1_score_val)) \n",
    "\n",
    "#area under ROC \n",
    "auc_evaluator = BinaryClassificationEvaluator()\n",
    "roc_value_rest = auc_evaluator.evaluate(predict_rest, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "roc_value_val = auc_evaluator.evaluate(predict_val, {auc_evaluator.metricName: \"areaUnderROC\"})\n",
    "print('The areaUnderROC on the train set is {:.2%}'.format(roc_value_rest))\n",
    "print('The areaUnderROC on the test set is {:.2%}'.format(roc_value_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "Create a parameter grid for tuning the model and perform cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random forest model\n",
    "rfparamGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.maxDepth,[3,5,10])\\\n",
    "    .addGrid(rf.numTrees, [20,50,75])\\\n",
    "    .addGrid(rf.impurity,[\"entropy\", \"gini\"])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=rfparamGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(rest)\n",
    "\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "predict_rest = cvModel.transform(rest)\n",
    "predict_validation = cvModel.transform(validation)\n",
    "\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(predict_rest)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(predict_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Gradient-Boosted Trees model\n",
    "gbparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [2, 5, 10])\n",
    "             .addGrid(gbt.maxBins, [10, 20, 40])\n",
    "             .addGrid(gbt.maxIter, [5, 10, 20])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for train set after CV  is 0.8100031959092361\n",
      "The area under ROC for test set after CV  is 0.65625\n"
     ]
    }
   ],
   "source": [
    "# Create 3-fold CrossValidator\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=gbparamGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(rest)\n",
    "\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "predict_rest = cvModel.transform(rest)\n",
    "predict_validation = cvModel.transform(validation)\n",
    "\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(predict_rest)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(predict_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score on the train set is 84.22%\n",
      "The F1 score on the test set is 81.73%\n"
     ]
    }
   ],
   "source": [
    "#F1 score\n",
    "f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "f1_score_rest = f1_score_evaluator.evaluate(predict_rest.select(col('label'), col('prediction')))\n",
    "f1_score_val = f1_score_evaluator.evaluate(predict_val.select(col('label'), col('prediction')))\n",
    "print('The F1 score after CV on the train set is {:.2%}'.format(f1_score_rest))\n",
    "print('The F1 score after CV on the test set is {:.2%}'.format(f1_score_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work: Testing selective sampling method\n",
    "Because the training data set is imbalanced with more 0 labeled rows than 1, here I wanted to try if randomly selecting the same number of 0 rows as 1 rows might improve the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of stayed and churned users\n",
    "df_model.groupBy('label').agg(countDistinct('userId').alias('user_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random selection of same number of 0 labeled rows as the 1 labeled rows\n",
    "df_model.where(df_model.label == \"0\").orderBy(rand()).limit(57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
